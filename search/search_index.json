{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bachelor Thesis \u2014 Lukas Johannes M\u00f6ller","text":"<p>University of Applied Sciences Kiel \u2014 Fachbereich Wirtschaft Degree Program: Wirtschaftsinformatik (B.Sc.) Thesis Topic: Coming Soon Supervisor: Prof. Dr. Christian Krauss Second Examiner: Prof. Dr. Stephan Schneider  </p>"},{"location":"#overview","title":"\ud83d\udcda Overview","text":"<p>Dieses Dokument enth\u00e4lt den aktuellen Fortschritt meiner Bachelorarbeit sowie alle Zwischenergebnisse, Literatur\u00fcbersichten und Arbeitsmaterialien.  </p>"},{"location":"#current-progress","title":"\u2705 Current Progress","text":"Datum Fortschritt 30.04.2025 Thesis Anmeldung eingereicht 28.04.2025 Hive Sensor Instalaltion l\u00e4uft \u00fcber Docker, GCP ist konfiguriert 19.04.2025 Madcat Container L\u00e4uft 3.04.2025 T-Pot aufgesetzt 24.03.2025 Grobe Gliederung und erste Konzeptskizze erstellen 22.03.2025 M\u00f6gliche Forschungsl\u00fccken sind skizziert. 22.03.2025 Identifizierte Literatur bewertet und Kernergebnisse zusammengefasst 22.03.2025 Erweiterte Literaturrecherche durchgef\u00fchrt 18.03.2025 Erste Auswahl relevanter Journals abgeschlossen"},{"location":"#woran-ich-gerade-arbeite","title":"\ud83d\udcc5 Woran ich gerade arbeite","text":"<ul> <li> Implementierung RL-Agent auf Hive</li> <li> Anbindung RL-Agent an Cluster VM und sicherstellung Deployment m\u00f6glich</li> </ul>"},{"location":"#geplant-backlog","title":"\u23f3 Geplant (Backlog)","text":"<ul> <li> Netzwerk in Google Cloud visualisieren, inklusive Segmentierung und Firewallregeln</li> <li> Google Cloud neu aufstzen mit naming Convention und richtiger Segmentierung und validierten Firewall Regeln </li> <li> Passendes Journal ausw\u00e4hlen (mit guten Ver\u00f6ffentlichungsaussichten)  </li> <li> Mit den Einreichungs- und Formatierungsrichtlinien des Journals vertraut machen  </li> <li> Struktur und Layout des Papers anhand der Vorgaben und des Konzepts definieren  </li> <li> Testen von Prof. Dr. Schneiders AI-Server Setup mit meinem Modell  </li> <li> Entwicklung und Verfeinerung des Deep-Learning-Modells gem\u00e4\u00df Forschungsfrage  </li> <li> Ggf. Durchf\u00fchrung vergleichender Analysen oder vertiefter Implementationen  </li> <li> Ergebnisse f\u00fcr die Journal-Einreichung aufbereiten  </li> <li> Paper zur Publikationspr\u00fcfung einreichen  </li> </ul>"},{"location":"#kontakt","title":"\ud83d\udcec Kontakt","text":"<p>Bei Fragen oder Feedback: Johannes M\u00f6ller lukas.j.moeller@student.fh-kiel.de  </p> <p>Letzte Aktualisierung: 18.03.2025</p>"},{"location":"documentation/","title":"Documentation Overview","text":"<p>Welcome to the detailed documentation.</p>"},{"location":"documentation/#sections","title":"\ud83d\udcd6 Sections","text":"<ul> <li>Related Work</li> <li>Journals</li> <li>Thoughts</li> <li>Concept I</li> </ul>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/","title":"Unsupervised Multi-Layer Anomaly Detection and Attack Chain Extraction in Unstructured Honeypot Logs","text":"<p>A Hybrid Deep Learning Framework for Modeling Complex Attacker Behavior on Unlabeled Production Data </p>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#1-introduction","title":"1. Introduction","text":"<p>Honeypot systems are widely deployed in cybersecurity research and defense operations to attract and record attacker behavior in controlled environments. These systems generate large volumes of highly unstructured, noisy, and heterogeneous log data. Detecting anomalies within these logs is crucial for identifying stealthy or previously unknown attacks. However, this task becomes significantly more complex when working with unlabeled production datasets, such as the one provided by the German Federal Office for Information Security (BSI), where no predefined ground truth or attack annotations are available.  </p> <p>This paper presents a novel unsupervised framework designed to not only detect anomalous behavior in unstructured honeypot logs but also to autonomously reconstruct potential attack chains by grouping related sessions and log entries. Our approach addresses both detection and interpretability challenges in environments where labeling is impractical or impossible.  </p>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#2-related-work-and-research-gap","title":"2. Related Work and Research Gap","text":"<ul> <li>DeepLog (2017) models log sequences with LSTMs but relies on structured templates and does not scale to unstructured or cross-session patterns.  </li> <li>AutoLog (2021) uses autoencoders for anomaly detection but ignores temporal and cross-session relationships.  </li> <li>LogBERT (2021) applies transformer-based sequence modeling but depends on predefined templates and does not handle multi-session attacker behavior.  </li> <li>Clustering-based methods detect attack patterns but focus on static flow-level metrics and do not leverage deep embeddings or time-based behavior modeling.  </li> </ul> <p>Identified Research Gap: To date, no unsupervised framework exists that: - Operates on unstructured, unlabeled honeypot log data, - Models attacker behavior across line-level, session-level, and cross-session layers, - Incorporates temporal modeling across multiple sessions to detect distributed attacks, - And extracts coherent attack chains from anomaly signals without ground-truth annotations.</p>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#3-research-objectives","title":"3. Research Objectives","text":"<ul> <li> <p>Develop a hybrid deep learning framework that integrates:  </p> <ul> <li>Line-level autoencoder for anomaly detection in individual log entries,  </li> <li>Session-level sequential modeling using LSTM to detect abnormal behavior patterns within sessions,  </li> <li>Time-based LSTM for modeling sequential patterns across multiple sessions over time,  </li> <li>A cross-session attention layer to highlight suspicious aggregated patterns.  </li> </ul> </li> <li> <p>Address the absence of labeled data by:  </p> <ul> <li>Operating fully unsupervised,  </li> <li>Using anomaly scores and embeddings to autonomously cluster sessions and reconstruct attack chains.  </li> </ul> </li> <li> <p>Enable explainability and analyst support by:  </p> <ul> <li>Providing log line-level anomaly scores,  </li> <li>Visualizing attention weights across sessions and time,  </li> <li>Outputting clustered groups of logs and sessions that constitute suspected attack chains.</li> </ul> </li> </ul>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#4-proposed-framework-architecture","title":"4. Proposed Framework Architecture","text":"<pre><code>graph TD\n    Input[\"Unlabeled Honeypot Log Data\"] --&gt; AE[\"Line-level Autoencoder (per line)\"]\n    AE --&gt; AE_Out[\"AE anomaly scores &amp; embeddings\"]\n    AE_Out --&gt; LSTM[\"Session-level LSTM (per session)\"]\n    LSTM --&gt; SessEmb[\"Session embeddings\"]\n    SessEmb --&gt; TimeLSTM[\"Time-based LSTM (sequence of session embeddings)\"]\n    TimeLSTM --&gt; Attention[\"Cross-session Attention Layer\"]\n    AE_Out --&gt; Fusion[\"Fusion Layer\"]\n    LSTM --&gt; Fusion\n    Attention --&gt; Fusion\n    Fusion --&gt; Chains[\"Attack Chain Extraction &amp; Clustering\"]\n    Chains --&gt; Output[\"Unified Anomaly Score &amp; Attack Chain Reports\"]</code></pre>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#key-components","title":"Key Components:","text":"<ul> <li>Line-level Autoencoder: Detects content anomalies in individual log lines.  </li> <li>Session-level LSTM: Captures sequential behavior anomalies within each session.  </li> <li>Time-based LSTM: Models ordered sequences of session embeddings over time to detect distributed attack chains.  </li> <li>Cross-session Attention Layer: Highlights aggregated suspicious behaviors across sessions and time.  </li> <li>Fusion Layer: Combines anomaly signals from line-level, session-level, and time-level.  </li> <li>Attack Chain Extraction: Clusters session groups forming coherent attack chains.  </li> </ul>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#5-future-work-toward-adaptive-honeypots","title":"5. Future Work: Toward Adaptive Honeypots","text":"<p>In the future, detected attack chains and time-based anomalies could be used to make honeypot systems adaptive. Dynamic configuration adjustments might include: - Extending session timeouts, - Changing simulated system fingerprints or banners, - Creating decoy assets that provoke attacker responses.  </p> <p>A reinforcement learning loop could optimize these adjustments based on the novelty and richness of attacker interactions, guided by embeddings and time-based anomaly detection signals.</p>"},{"location":"documentation/Unsupervised_Multi-Layer_Anomaly_Detection_in_Unstructured_Honeypot_Logs_Across_Sessions_and_Time/#6-conclusion","title":"6. Conclusion","text":"<p>The proposed framework extends beyond single-session analysis and incorporates time-based LSTM modeling to capture attacker activity patterns across sessions and over time. Combined with unsupervised clustering, this enables autonomous attack chain reconstruction and lays the groundwork for future adaptive honeypot behavior.</p>"},{"location":"documentation/concept1/","title":"Unsupervised Multi-Model Anomaly Detection and Attack Chain Versioning in Adaptive Honeynets","text":"<p>Early Threat Recognition from First-Flight Data with Resource-Efficient Container Orchestration</p>"},{"location":"documentation/concept1/#1-einleitung-und-problemstellung","title":"1. Einleitung und Problemstellung","text":""},{"location":"documentation/concept1/#11-motivation","title":"1.1 Motivation","text":"<p>Moderne Honeypot-Systeme sind in der Lage, umfangreiche Datenmengen zu erzeugen, die f\u00fcr die Analyse von Angriffsmustern genutzt werden k\u00f6nnen. Allerdings haben sie nach wie vor erhebliche Schw\u00e4chen, insbesondere im Umgang mit adaptiven Angreifern. Diese Angreifer, oftmals als Bots bezeichnet, passen ihre Taktiken kontinuierlich an, um der Erkennung durch statische Systeme zu entgehen. W\u00e4hrend klassische Honeypots auf die Erkennung isolierter Angriffe ausgerichtet sind, bleiben komplexe, dynamische Angreifer, die Angriffsketten \u00fcber mehrere Sessions hinweg aufbauen, h\u00e4ufig unerkannt. </p> <p>Ein weiteres zentrales Problem bei der Anwendung von Honeypots ist die Skalierbarkeit und Ressourcennutzung. In realen Angriffsszenarien, in denen mehrere Angriffe gleichzeitig und \u00fcber l\u00e4ngere Zeitr\u00e4ume hinweg stattfinden, ist es entscheidend, dass Honeypot-Systeme dynamisch reagieren und Ressourcen effizient verwalten. Die Notwendigkeit, auf Echtzeit-Angriffe zu reagieren, ohne in Ressourcenspitzen zu geraten, stellt eine weitere Herausforderung dar.</p> <p>Die vorliegende Arbeit zielt darauf ab, ein adaptives Honeynet-Framework zu entwickeln, das sowohl die Erkennung adaptiver Angreifer (insbesondere Bots) als auch die Versionierung von Angriffsketten erm\u00f6glicht. Gleichzeitig soll das System durch intelligente Container-Orchestrierung die Ressourcennutzung optimieren und die Effizienz bei der Emulation von Angriffen sicherstellen, insbesondere bei hohem Traffic oder langfristigen Angriffsszenarien.</p>"},{"location":"documentation/concept1/#12-forschungsfragen","title":"1.2 Forschungsfragen","text":"<p>Die folgenden Forschungsfragen ergeben sich aus den praktischen Herausforderungen bei der Erkennung und Analyse von adaptiven Angreifern sowie der effizienten Ressourcennutzung in Honeypots:</p> <ol> <li> <p>Wie kann ein hybrides Deep-Learning-Modell, das Autoencoder (AE), Long Short-Term Memory (LSTM) Netzwerke und Graph Neural Networks (GNN) kombiniert, dazu beitragen, adaptive Angreifer zu erkennen und Angriffsketten zu versionieren, die sich \u00fcber mehrere Sessions hinweg entwickeln?</p> <ul> <li>Forschungsl\u00fccke: W\u00e4hrend bestehende Systeme teilweise Angreifer auf Basis von einzelnen Angriffsmustern oder Datenstr\u00f6men erkennen, wird die F\u00e4higkeit zur langfristigen, dynamischen Erkennung adaptiver Angreifer und Versionierung von Angriffsketten selten ber\u00fccksichtigt. Das vorgestellte System adressiert diese L\u00fccke, indem es First-Flight-Daten kombiniert mit mehreren modellen zur Anomalieerkennung und einer Attack-Chain-Versionierung nutzt, um Verhaltenstrends von Angreifern \u00fcber l\u00e4ngere Zeitr\u00e4ume hinweg zu verfolgen.</li> </ul> </li> <li> <p>Wie kann ein Reinforcement-Learning-gest\u00fctztes System die Container-Orchestrierung in Honeypots effizient steuern, um die Ressourcennutzung zu optimieren und gleichzeitig die Erkennungsqualit\u00e4t und Versionierung von Angreifern nicht zu beeintr\u00e4chtigen?</p> <ul> <li>Forschungsl\u00fccke: Viele bestehende Systeme emulieren Angriffe ohne R\u00fccksicht auf die Ressourcenkapazit\u00e4ten, was zu einer hohen Belastung des Systems f\u00fchrt, insbesondere bei hochfrequenten Angriffen. Das vorgestellte Framework nutzt Reinforcement Learning (RL) zur ressourcensensitiven Steuerung von Container-Spawns, um sicherzustellen, dass nur relevante Angriffsszenarien emuliert werden und gleichzeitig Ressourcen effizient genutzt werden. Dieses System k\u00f6nnte die Echtzeitreaktionsf\u00e4higkeit erheblich steigern und die Fehlertoleranz verbessern.</li> </ul> </li> <li> <p>Wie k\u00f6nnen wir ein System zur erkl\u00e4rbaren Anomalieerkennung (XAI) entwickeln, das es erm\u00f6glicht, sowohl die Erkennungsentscheidungen als auch die Entwicklung von Angriffsketten f\u00fcr Sicherheitsanalysten nachvollziehbar zu machen?</p> <ul> <li>Forschungsl\u00fccke: Erkl\u00e4rbarkeit ist ein kritischer Faktor bei der Implementierung von Sicherheitsl\u00f6sungen, um Vertrauen und Transparenz zu gew\u00e4hrleisten. In bisherigen Ans\u00e4tzen wird die Erkl\u00e4rung von Erkennungen meist vernachl\u00e4ssigt. Diese Arbeit strebt an, eine explainable AI (XAI) zu integrieren, um sicherzustellen, dass die Entscheidungsprozesse des Modells f\u00fcr Sicherheitsexperten verst\u00e4ndlich sind. Dies ist besonders wichtig, um bei komplexen, dynamischen Angriffen wie APT-Angriffen oder langfristigen Botnet-Infiltrationen die Angriffsschritte und deren Zusammenh\u00e4nge transparent zu machen.</li> </ul> </li> </ol>"},{"location":"documentation/concept1/#13-hypothese","title":"1.3 Hypothese","text":"<p>Die Haupthypothese dieser Arbeit lautet:</p> <p>Ein mehrschichtiges Modell aus Autoencodern (AE), Long Short-Term Memory Netzwerken (LSTM) und Graph Neural Networks (GNN) kann in Kombination mit Reinforcement Learning und Explainable AI in einem adaptiven Honeynet-Framework die Fr\u00fcherkennung von adaptiven Angreifern, die Versionierung von Angriffsketten sowie eine effiziente Ressourcennutzung erm\u00f6glichen. Dieses Modell wird es erm\u00f6glichen, nicht nur einzelne Angriffe fr\u00fchzeitig zu erkennen, sondern auch die Entwicklung von Angreifern \u00fcber mehrere Sessions hinweg zu verfolgen, was zu einer nachhaltigeren und leistungsf\u00e4higeren Sicherheitsl\u00f6sung f\u00fchrt. Dabei wird die Ressourcenschonung durch die intelligente Container-Orchestrierung des RL-Moduls optimiert, ohne die Qualit\u00e4t der Angriffsbeobachtungen zu beeintr\u00e4chtigen.</p>"},{"location":"documentation/concept1/#20-verwandte-arbeiten-und-abgrenzung","title":"2.0 Verwandte Arbeiten und Abgrenzung","text":"<p>Forschungsarbeiten zur Anomalieerkennung im Kontext von Honeypots, Netzwerkverkehr und Logs lassen sich grob in drei Kategorien einteilen:</p> <p>(1) klassische Honeypot-Frameworks (2) Deep-Learning-basierte Anomalieerkennungssysteme (3) unsupervised/multimodale Netzwerkmodelle</p> <p>Obwohl jede dieser Gruppen Teilaspekte adressiert, fehlt bislang ein System, das dynamisch auf die Entwicklung von Angreifern reagiert, die Versionierung von Angriffsstrukturen (wie Attack Chains) erm\u00f6glicht und gleichzeitig Ressourcen effizient nutzt.</p>"},{"location":"documentation/concept1/#21-klassische-honeypot-systeme","title":"2.1 Klassische Honeypot-Systeme","text":"<ul> <li> <p>T-Pot bietet eine Vielzahl von Emulationsdiensten, basiert jedoch auf statisch laufenden Containern. Die fehlende dynamische Anpassung an den Traffic und die Relevanz der einzelnen Emulationen f\u00fchrt zu einer ineffizienten Ressourcennutzung. Eine Versionierung von Angriffsketten oder die Verfolgung von sich entwickelnden Angreifern wird nicht ber\u00fccksichtigt.</p> </li> <li> <p>MADCAT ist ein Low-Interaction-System, das auf die Extraktion von Rohdaten aus der Netzwerkinfrastruktur fokussiert. Im Gegensatz zu traditionellen Honeypots mit komplexen Antwortlogiken bietet MADCAT eine skalierbare Erfassungsmethode. Doch auch hier fehlt die M\u00f6glichkeit zur tiefgehenden Erkennung und Versionierung von Angriffsketten, die in adaptiven Bedrohungsszenarien notwendig ist.</p> </li> <li> <p>UNADA schl\u00e4gt eine Clustering-basierte Anomalieerkennung vor, jedoch ohne tiefere Modellarchitektur oder die F\u00e4higkeit zur dynamischen Emulationssteuerung. Es werden keine Mechanismen zur kontinuierlichen Verfolgung und Versionierung von sich entwickelnden Angriffen integriert.</p> </li> <li> <p>Hybrid IDS mit Honeypots verfolgt das Konzept, Honeypots in ein IDS zu integrieren, jedoch bleibt es ohne detaillierte Implementierung und liefert keine Erkenntnisse zur dynamischen Reaktion auf sich entwickelnde Angriffe oder zur Versionierung von Angriffsketten.</p> </li> </ul> <p>Forschungsl\u00fccke: Es gibt kein Honeypot-System, das dynamisch Angriffsketten versioniert, die sich \u00fcber mehrere Interaktionen hinweg entwickeln, und gleichzeitig adaptive, sich \u00e4ndernde Bots erkennt und deren Entwicklung \u00fcber Zeit verfolgt.</p>"},{"location":"documentation/concept1/#22-deep-learning-fur-anomalieerkennung","title":"2.2 Deep Learning f\u00fcr Anomalieerkennung","text":"<ul> <li> <p>DeepLog ist auf strukturierte Logs angewiesen und verwendet LSTMs zur Erkennung von Anomalien. Diese Methode ist jedoch nicht in der Lage, komplexe Angriffsketten \u00fcber mehrere Sessions hinweg zu erkennen oder sich entwickelnde Bots zu verfolgen. Zudem fehlt die M\u00f6glichkeit zur dynamischen Container-Orchestrierung, die es erm\u00f6glicht, Ressourcen je nach Angriffskomplexit\u00e4t zu steuern.</p> </li> <li> <p>D-PACK nutzt Autoencoder zur Fr\u00fcherkennung von Angriffen, die auf den ersten Bytes eines Netzwerks basieren. Diese Methode ist jedoch auf bestimmte Angriffsarten beschr\u00e4nkt und bietet keine M\u00f6glichkeit zur Erkennung von adaptiv agierenden Angreifern oder deren Versionierung. Au\u00dferdem fehlt eine tiefere Analyse von Angriffsketten.</p> </li> <li> <p>AutoLog kombiniert Entropie-basiertes Scoring und Autoencoder, bietet aber keine Echtzeitf\u00e4higkeit und ist nicht auf Netzwerkverkehr ausgerichtet. Hier fehlen ebenfalls Mechanismen zur Verfolgung und Versionierung von Angriffen, die sich \u00fcber Zeit ver\u00e4ndern.</p> </li> </ul> <p>Forschungsl\u00fccke: Aktuelle Deep-Learning-Ans\u00e4tze sind entweder auf spezifische Angriffstypen fokussiert oder nicht in der Lage, sich entwickelnde Angreifer zu erkennen und deren Taktiken kontinuierlich zu verfolgen. Ein tiefgehendes Modell, das auch auf First-Flight-Daten reagiert, ist bisher nicht verf\u00fcgbar.</p>"},{"location":"documentation/concept1/#23-multimodale-und-unuberwachte-netzwerkansatze","title":"2.3 Multimodale und un\u00fcberwachte Netzwerkans\u00e4tze","text":"<ul> <li> <p>FedNIDS basiert auf federierten DNNs, die auf rohen Paketdaten trainiert werden, jedoch auf \u00fcberwachten Ans\u00e4tzen beruhen, was sie f\u00fcr un\u00fcberwachte Honeypot-Umgebungen ungeeignet macht. Zudem erfordert das System hohe Rechenressourcen und bietet keine Echtzeitf\u00e4higkeit zur Versionierung von Angriffsstrukturen.</p> </li> <li> <p>ICMLA 2023 (Raw Packet Transformers) setzt ByT5-Transformers auf rohen Netzwerkdaten ein, jedoch hat dieses Modell eine hohe Rechenlast und bietet keine erkl\u00e4rbare Entscheidungsfindung. Es fehlt die F\u00e4higkeit zur dynamischen Versionierung und zur Modellierung von Angriffsverhalten im Zeitverlauf.</p> </li> </ul> <p>Forschungsl\u00fccke: Es gibt keine Systeme, die eine dynamische Angriffs-Ketten-Versionierung erm\u00f6glichen und dabei gleichzeitig Ressourceneffizienz bei der Erkennung von Angreifern sicherstellen. Dein Framework zielt darauf ab, Bots zu versionieren und zu verfolgen, wenn diese sich im Laufe der Zeit anpassen \u2013 ein bisher wenig erforschtes Gebiet.</p>"},{"location":"documentation/concept1/#24-systemvergleichstabelle","title":"2.4 Systemvergleichstabelle","text":"Funktion / System T-Pot [1] DeepLog [2] UNADA [3] D-PACK [4] FedNIDS [5] Unser System Adaptive Containersteuerung (RL, Ressourcen-aware) \u2716 \u2716 \u2716 \u2716 \u2716 \u2705 First-Flight-Auswertung \u2716 \u2716 \u2716 \u2705 \u2716 \u2705 Multi-Modell-Anomalieerkennung (AE+LSTM+GNN) \u2716 LSTM \u2716 AE DNN \u2705 Dynamische Angriffsketten-Versionierung \u2716 \u2716 teilw. \u2716 \u2716 \u2705 Unsupervised Training \u2716 teilw. \u2705 \u2705 \u2716 (supervised) \u2705 Explainability (XAI) \u2716 \u2716 \u2716 \u2716 \u2716 \u2705 STIX 2.1 / SIEM-Kompatibilit\u00e4t \u2716 \u2716 \u2716 \u2716 \u2716 \u2705 Verarbeitung unstrukturierter Netzwerk-Payload \u2716 Logs \u2716 Logs \u2716 Flows \u2705 \u2705 \u2705 Ressourceneffizienter Betrieb bei hohem Traffic \u2716 \u2716 \u2716 teilw. \u2716 \u2705"},{"location":"documentation/concept1/#25-fazit-der-analyse","title":"2.5 Fazit der Analyse","text":"<p>Die bestehenden Systeme liefern wertvolle Beitr\u00e4ge zur Erkennung von Angriffen, doch keines dieser Systeme bietet eine integrierte L\u00f6sung, die sich entwickelnde Angreifer erkennt und deren Angriffsketten versioniert. Dein Ansatz zur dynamischen Verfolgung von Angriffsketten, kombiniert mit der Versionierung von sich anpassenden Bots, stellt eine neuartige und notwendige Erweiterung bestehender Methoden dar. Es erm\u00f6glicht nicht nur die Erkennung von Angriffen in Echtzeit, sondern auch die kontinuierliche Beobachtung und Anpassung an sich entwickelnde Bedrohungen.</p>"},{"location":"documentation/concept1/#3-systemarchitektur","title":"3. Systemarchitektur","text":""},{"location":"documentation/concept1/#31-komponentenubersicht","title":"3.1 Komponenten\u00fcbersicht","text":"<ul> <li>Sensoring Layer: Netzwerkweiterleitung \u00fcber T7-Proxy, TLS-Termination und Protokollklassifikation.</li> <li>First-Flight Modul: Erfasst passive First-Flight-Verbindungen von MADCAT ohne aktive Emulation. Die ersten Sitzungsmerkmale wie SYN-Pakete, Protokollheader, Payload-Fragmente und Timing-Indikatoren werden extrahiert, sobald die ersten Interaktionspakete (TCP, UDP, ICMP, RAW) empfangen werden.</li> <li>First-Flight Deep Learning Modul: L\u00e4dt und analysiert First-Flight-Logs aus dem ELK-Stack. Mithilfe eines Autoencoders und einem heuristischen Scoring-Mechanismus werden vielversprechende First-Flight-Sitzungen identifiziert, die zur weiteren Analyse in das System aufgenommen werden.</li> <li>RL-Dispatcher: Bei Identifikation einer vielversprechenden Sitzung durch das Deep Learning Modul wird ein Auftrag an den Kubernetes-Dispatcher gesendet, um einen Pod zu erstellen. Der Pod erh\u00e4lt eine Quell-IP und den zugeh\u00f6rigen Port f\u00fcr die Weiterleitung von MADCAT.</li> <li>Pod-Analyse: Der erstellte Pod fordert \u00fcber die MADCAT-API die Quell-IP weiterzuleiten und f\u00fchrt eine tiefere Analyse der Sitzungsdaten durch. Das zweite Deep Learning Modul, das auf einem Single-Line-Autoencoder basiert, untersucht zeitliche Zusammenh\u00e4nge und berechnet den Fusionsscore f\u00fcr die Session.</li> <li>Fusionsmodul: Berechnet den Fusionsscore aus den Ergebnissen der Pod-Analyse, der aus den verschiedenen Anomalieerkennungspfaden (AE, LSTM, GNN) hervorgeht.</li> <li>Logger &amp; Analyzer: Persistiert die Sitzungen, vergleicht Embeddings und rekonstruier Angriffsketten durch Clustering.</li> <li>Ressourcenkostenbewertung (RL): Der Reward wird nach der Pod-Analyse an das RL-Modul zur\u00fcckgegeben, basierend auf der Effizienz und den analysierten Daten der Containeremulation.</li> </ul>"},{"location":"documentation/concept1/#32-formeller-datenfluss","title":"3.2 Formeller Datenfluss","text":"<p>Der formelle Datenfluss unseres Frameworks kann wie folgt beschrieben werden:</p> \\[ X := f_{\\text{FF}}(\\text{pkt}_{\\text{init}}) \\Rightarrow V := \\phi(X) \\Rightarrow \\text{RL}(V) \\Rightarrow \\text{Spawn}(C_i) \\,|\\, \\text{Drop} \\Rightarrow \\text{Pod-Analyse} \\Rightarrow \\text{Fusionsscore berechnen} \\Rightarrow \\delta(s_{\\text{fusion}}, T) \\] <ul> <li>\\( f_{\\text{FF}}(\\text{pkt}_{\\text{init}}) \\): Erfasst First-Flight-Daten basierend auf den ersten Interaktionspaketen. Diese initialen Sitzungsmerkmale werden an den Feature Extractor \\( \\phi(X) \\) weitergeleitet.</li> <li>Das RL-Modul: Bewertet die Feature-Vektoren und entscheidet, ob ein Container gestartet, verz\u00f6gert oder verworfen wird.</li> <li>Pod-Analyse: Nach dem Start des Containers wird eine detaillierte Analyse der Logs im Pod durchgef\u00fchrt und der Fusionsscore berechnet, basierend auf den an den ELK-Stack gesendeten Logdaten.</li> </ul>"},{"location":"documentation/concept1/#33-first-flight-modul-und-container-orchestrierung","title":"3.3 First-Flight-Modul und Container-Orchestrierung","text":""},{"location":"documentation/concept1/#first-flight-erfassung","title":"First-Flight-Erfassung","text":"<p>Das First-Flight Modul erfasst die ersten Verbindungsdaten, die ohne aktive Emulation protokolliert werden. Diese Daten umfassen:</p> <ul> <li>Verbindungsaufbau (TCP-SYN \u2192 ACK \u2192 Payload)</li> <li>Protokollkennungen (z.\u202fB. HTTP-Header, SSH-Authentifizierung)</li> <li>Fr\u00fche Payloads (z.\u202fB. Passworteingabe, Command Stubs)</li> <li>Netzwerkmetadaten (z.\u202fB. Timing, TTL, TCP-Optionen)</li> </ul> <p>Die Erfassung erfolgt nicht zeitgesteuert, sondern basierend auf dem Eingang aller erforderlichen ersten Interaktionspakete, die bei einer passiven Beobachtung vollst\u00e4ndig erfasst werden.</p>"},{"location":"documentation/concept1/#container-orchestrierung","title":"Container-Orchestrierung","text":"<p>Die Entscheidung zur Container-Orchestrierung erfolgt vor der Pod-Analyse, basierend auf den von First-Flight extrahierten Feature-Vektoren und der Bewertung des RL-Moduls. Der Fusionsscore wird jedoch erst nach der Pod-Analyse berechnet und beeinflusst sp\u00e4tere Entscheidungen zur Ressourcenzuweisung, z.\u202fB. ob der Container gestoppt oder fortgesetzt wird.</p>"},{"location":"documentation/concept1/#4-anomaliefusion-entscheidungsmodell","title":"4. Anomaliefusion &amp; Entscheidungsmodell","text":""},{"location":"documentation/concept1/#41-herleitung-der-fusionsformel","title":"4.1 Herleitung der Fusionsformel","text":"<p>Die Fusion von Anomalie-Scores aus verschiedenen Modellen (Autoencoder, LSTM, GNN) erfordert eine Aggregationsmethode, die sowohl Synergieeffekte erkennt als auch extreme Einzelwerte d\u00e4mpfen kann. Die gew\u00e4hlte Formel basiert auf einer exponentiell gewichteten Multiplikation:</p> \\[ s_{fusion} = ((s_{line}+1)^\\alpha \\cdot (s_{session}+1)^\\beta \\cdot (s_{graph}+1)^\\gamma) - 1 \\]"},{"location":"documentation/concept1/#begrundung","title":"Begr\u00fcndung:","text":"<ul> <li>Die additive Verschiebung um +1 verhindert Degeneration durch Nullwerte in einzelnen Scores.</li> <li>Die Multiplikation erzeugt Synergieeffekte: Nur wenn mehrere Komponenten gleichzeitig eine hohe Anomalie erkennen, wird das Gesamtsignal signifikant.</li> <li>Die Exponenten \\(\\alpha, \\beta, \\gamma\\) dienen zur priorisierten Gewichtung und erlauben eine Feinjustierung je nach Modellvertrauen.</li> </ul>"},{"location":"documentation/concept1/#alternative-fusionsansatze-vergleichbar-in-evaluation","title":"Alternative Fusionsans\u00e4tze (Vergleichbar in Evaluation):","text":"Methode Formel Eigenschaft Weighted Sum \\( \\alpha s_{line} + \\beta s_{session} + \\gamma s_{graph} \\) Linear, interpretierbar Log-Fusion \\( \\log(1 + \\alpha s_{line}) + \\log(1 + \\beta s_{session}) + \\log(1 + \\gamma s_{graph}) \\) Robust gegen Ausrei\u00dfer Softmax-Normalisierung \\( \\frac{e^{\\alpha s_{line}} + e^{\\beta s_{session}} + e^{\\gamma s_{graph}}}{3} \\) Verst\u00e4rkt dominante Anomalien Multiplikation (unsere Methode) \\( ((s+1)^\\alpha...) - 1 \\) Belohnt gleichzeitige Auff\u00e4lligkeiten <p>Ein systematischer Vergleich dieser Varianten erfolgt in der Evaluation durch Ablation Studies (vgl. Abschnitt 9).</p>"},{"location":"documentation/concept1/#411-adaptive-gewichtungsmatrix-nach-protokolltyp","title":"4.1.1 Adaptive Gewichtungsmatrix nach Protokolltyp","text":"<p>Die Fusionsformel nutzt eine adaptive Gewichtungsmatrix basierend auf dem Protokolltyp \\( P \\) und den charakteristischen Merkmalen der Session \\( F \\), die die Modellrelevanz bestimmen:</p> \\[ (\\alpha, \\beta, \\gamma) = f(P, F) \\]"},{"location":"documentation/concept1/#funktion-fp-f-regelbasiert-lernfahig","title":"Funktion \\( f(P, F) \\): Regelbasiert &amp; lernf\u00e4hig","text":"<p>Die Gewichtungen werden durch eine Kombination von regelbasierten Zuordnungen und einem Meta-Modell erzeugt, das durch das Training optimiert wird:</p> \\[ f(P, F) := \\text{MLP}_{\\text{meta}}([\\text{enc}(P); \\text{stat}(F)]) \\] <ul> <li>\\( \\text{enc}(P) \\): Gelerntes Embedding f\u00fcr den Protokolltyp</li> <li>\\( \\text{stat}(F) \\): Statistische Merkmale der Session (z.\u202fB. Entropie, TCP-Flags, Paketanzahl, Payload-L\u00e4nge)</li> <li>\\( \\text{MLP}_{\\text{meta}} \\): Feedforward-Netzwerk, das die Gewichtungen \\( \\alpha, \\beta, \\gamma \\) berechnet und normalisiert, sodass \\( \\alpha + \\beta + \\gamma = 1 \\)</li> </ul>"},{"location":"documentation/concept1/#412-vorteil","title":"4.1.2 Vorteil","text":"<ul> <li>Explizite Priorisierung je nach Angriffstyp/Verkehrsmuster</li> <li>Erm\u00f6glicht z.\u202fB. vollst\u00e4ndige GNN-Gewichtung bei SMTP-Chains oder AE+LSTM bei FTP</li> <li>Evaluierung erfolgt im Rahmen der Ablation (vgl. Abschnitt 9.3)</li> </ul>"},{"location":"documentation/concept1/#42-schwellenwertfunktion","title":"4.2 Schwellenwertfunktion","text":"\\[ \\delta(s_{fusion}, T) = \\begin{cases} 1 &amp; s_{fusion} \\geq T \\\\ 0 &amp; \\text{sonst} \\end{cases} \\] <p>Nur wenn \\( \\delta = 1 \\) wird ein Emulationscontainer gestartet. Dies reduziert unproduktive Emulationen.**</p>"},{"location":"documentation/concept1/#43-ressourcenkostenbewertung-reinforcement-learning","title":"4.3 Ressourcenkostenbewertung (Reinforcement Learning)","text":""},{"location":"documentation/concept1/#reward-funktion","title":"Reward-Funktion:","text":"\\[ R_t = \\frac{I_t^{\\text{neu}}}{I_t^{\\text{total}}} - \\lambda \\cdot C_t \\] <ul> <li>\\( I_t^{neu} \\): Neue Information (Embedding-Distanz zu Cluster-Zentrum &gt; \\( \\epsilon \\))</li> <li>\\( C_t \\): Laufzeitkosten (CPU, RAM, Zeit)</li> <li>\\( \\lambda \\): Regularisierungsparameter</li> </ul>"},{"location":"documentation/concept1/#beispiel","title":"Beispiel:","text":"<p>Gegeben seien: \\( I_t^{neu} = 0.2 \\), \\( I_t^{total} = 1.5 \\), \\( C_t = 0.1 \\), \\( \\lambda = 0.5 \\) $$ R_t = \\frac{0.2}{1.5} - 0.5 \\cdot 0.1 = 0.133 - 0.05 = 0.083 $$</p>"},{"location":"documentation/concept1/#rl-komponenten","title":"RL-Komponenten:","text":"<ul> <li>Zustand: \\( s_t = [s_{fusion}, t_{last}, r_{usage},\\dots] \\)</li> <li>Aktionen: \\( \\{ \\text{Spawn}, \\text{Delay}, \\text{Drop} \\} \\)</li> <li>Lernalgorithmus: PPO (Proximal Policy Optimization) mit kontinuierlichem Update</li> </ul>"},{"location":"documentation/concept1/#44-gnn-architektur-sensitivitatsanalyse-der-fusionsformel","title":"4.4 GNN-Architektur &amp; Sensitivit\u00e4tsanalyse der Fusionsformel","text":""},{"location":"documentation/concept1/#gnn-einsatz","title":"GNN-Einsatz","text":"<p>F\u00fcr die Analyse topologischer Sessionstrukturen verwenden wir ein heterogenes GNN-Modell auf Basis von Relational Graph Attention Networks (R-GAT). Dieses erlaubt eine differenzierte Modellierung von semantischen Kanten (z.\u202fB. TCP-Verbindung, Payload-Similarit\u00e4t, Portgleichheit) bei gleichzeitiger Gewichtung durch lernbare Attention-Koeffizienten. Der Eingabegraph \\( G = (V, E) \\) repr\u00e4sentiert dabei eine Session als Kommunikationsstruktur, wobei:</p> <ul> <li>Knoten: Rollen im Verkehr (IP-Endpunkte, Dienste, identifizierte Fingerprints)</li> <li>Kanten: Beziehungstypen mit Kategorien wie <code>conn_established</code>, <code>shared_payload_hash</code>, <code>service_similarity</code></li> </ul> <p>Der Output ist ein Session-Embedding \\( \\mathbf{e}_{graph} \\in \\mathbb{R}^d \\), dessen Abweichung vom Referenzverhalten den Score \\( s_{graph} \\) bestimmt. Vorteil der GAT-Struktur: sie bleibt robust gegen\u00fcber unvollst\u00e4ndiger Topologie und lernt gewichtete Kontexte auch bei sparsamen Verbindungen.</p>"},{"location":"documentation/concept1/#sensitivitatsanalyse-der-fusionsformel","title":"Sensitivit\u00e4tsanalyse der Fusionsformel","text":"<p>Die gew\u00e4hlte Fusionsformel $$ s_{fusion} = \\left( (s_{line}+1)^\\alpha \\cdot (s_{session}+1)^\\beta \\cdot (s_{graph}+1)^\\gamma \\right) - 1 $$ verst\u00e4rkt Synergieeffekte, kann jedoch bei niedrigem Score-Niveau einzelner Komponenten anf\u00e4llig f\u00fcr Instabilit\u00e4ten sein. Zur Sensitivit\u00e4tsanalyse untersuchen wir die partielle Ableitung nach einem Score, z.\u202fB. \\( \\frac{\\partial s_{fusion}}{\\partial s_{line}} \\), um Einflussbereiche zu quantifizieren:</p> \\[ \\frac{\\partial s_{fusion}}{\\partial s_{line}} = \\alpha (s_{line}+1)^{\\alpha-1} \\cdot (s_{session}+1)^\\beta \\cdot (s_{graph}+1)^\\gamma \\] <p>Die Ableitung zeigt, dass geringe Werte in \\( s_{line} \\) exponentiell verst\u00e4rkt werden, sobald die anderen Scores hoch sind. Daher f\u00fchren wir in der Implementation einen Clamp-Mechanismus ein, der einzelne Scores auf ein Intervall \\([0.01, 10]\\) begrenzt und so numerische Dominanzen verhindert. Zus\u00e4tzlich testen wir die Robustheit gegen schwankende Einzelwerte durch Monte-Carlo-Simulationen auf synthetischen Score-Sets, um Verl\u00e4ufe und Anf\u00e4lligkeiten zu evaluieren.</p>"},{"location":"documentation/concept1/#441-cross-session-graph-verteilte-angriffserkennung","title":"4.4.1 Cross-Session Graph: verteilte Angriffserkennung","text":"<p>Zur Erkennung korrelierter, verteilter Angriffe (z.\u202fB. APT, DDoS, verteilte Reconnaissance) erweitern wir das session-zentrierte GNN um einen globalen Cross-Session-Graphen \\( G^{\\text{global}} = (V, E) \\), wobei:</p> <ul> <li>Knoten \\( V \\): individuelle Sessions (nicht einzelne IPs!)</li> <li>Kanten \\( E \\): korrelative Beziehungen, z.\u202fB.:<ul> <li>gleiche Quell-IP</li> <li>\u00e4hnliche Payload-Hashes (Simhash &gt; 0.8)</li> <li>identische Zielports in kurzer Zeit (\u2264\u202f30s)</li> <li>Session-Embedding-Distanz \\( d &lt; \\epsilon \\)</li> </ul> </li> </ul> <p>Jede neue Session wird in diesen globalen Graph integriert, wodurch Angriffscluster auf h\u00f6herer Ebene sichtbar werden.</p>"},{"location":"documentation/concept1/#442-analyse","title":"4.4.2 Analyse","text":"<p>Wir wenden ein weiteres R-GAT-Modell auf \\( G^{\\text{global}} \\) an, das Topologie und semantische \u00c4hnlichkeit nutzt, um verteilte Anomalien zu detektieren. Das Ergebnis ist ein zus\u00e4tzlicher Anomalie-Score \\( s_{cross} \\), der in die Fusion einflie\u00dft:</p> \\[ s_{fusion}^{\\text{final}} = \\left( ((s_{line}+1)^\\alpha \\cdot (s_{session}+1)^\\beta \\cdot (s_{graph}+1)^\\gamma) + s_{cross} \\right) - 1 \\] <p>\\( s_{cross} \\) wird nur ber\u00fccksichtigt, wenn die Session im globalen Graph mit \u2265\u202f2 Nachbarn verbunden ist (Verteilungsmerkmal erf\u00fcllt).</p>"},{"location":"documentation/concept1/#45-robustheit-gegen-adversarial-payloads","title":"4.5 Robustheit gegen adversarial payloads","text":"<p>Zielgerichtete Payload-Manipulationen, wie Fragmentierung, semantisch irrelevante Padding-Sequenzen oder zeitlich gestreckte Protokollinteraktionen, k\u00f6nnen die Anomalie-Scores absichtlich senken. Um dem zu begegnen, f\u00fchren wir einen Adversarial Robustness Score (ARS) ein, der die Stabilit\u00e4t der Entscheidung unter leicht ver\u00e4nderten First-Flight-Eingaben misst:</p> \\[ ARS = 1 - \\frac{1}{k} \\sum_{i=1}^k \\left| s_{fusion}^{(0)} - s_{fusion}^{(i)} \\right| \\] <p>Dabei bezeichnet \\( s_{fusion}^{(0)} \\) den Original-Score und \\( s_{fusion}^{(i)} \\) die Scores nach gezielten Mikrover\u00e4nderungen in Payload, Timing oder TCP-Feldern. Ein niedriger ARS (&lt;\u202f0.7) weist auf ein hohes Risiko adversarialer T\u00e4uschung hin.</p> <p>Zur Erh\u00f6hung der Robustheit setzen wir zus\u00e4tzlich auf:</p> <ul> <li>Payload-Normalisierung vor dem AE-Eingang (Entfernung redundanter Padding-Byte-Bl\u00f6cke)</li> <li>Sequenz-Augmentation im LSTM-Training (Jitter, Dropouts, Insertions)</li> <li>Graph-Dropout-Simulation in der GNN-Trainingsphase</li> <li>Zeitbasierte Nebenmerkmale wie Jitter-Spikes, Retransmission-Delay und Early-TCP-Resets als Zusatzfeatures Hier ist eine \u00fcberarbeitete Fassung mit pr\u00e4ziser wissenschaftlicher Fundierung und sinnvollen Erg\u00e4nzungen zu den offenen Punkten im Bereich First-Flight-Logik, Pod-Dispatching und RL-Policy-Verhalten. Der Stil passt sich deinem urspr\u00fcnglichen Paper-Entwurf an:</li> </ul>"},{"location":"documentation/concept1/#46-verhalten-der-rl-policy-bei-drop-entscheidungen","title":"4.6 Verhalten der RL-Policy bei \u201eDrop\u201c-Entscheidungen","text":"<p>Die Reinforcement-Learning-Policy kann f\u00fcr Sessions mit niedrigem Erkenntnispotenzial explizit die Aktion Drop w\u00e4hlen. In diesem Fall erfolgt kein Container-Spawn \u2013 jedoch wird die Session nicht verworfen, sondern in einen passiven Beobachtungsmodus \u00fcberf\u00fchrt:</p> <ul> <li>Logging wird fortgesetzt mit reduzierter Frequenz (Sampling-Mode)</li> <li>Scores &lt;\u202fT werden in eine \u201eLow-Signal-Trajektorie\u201c aufgenommen</li> <li>Falls \u00fcber die Zeit (z.\u202fB. innerhalb von 30 Sekunden) ein Score-Spike eintritt, kann die Session nachtr\u00e4glich reaktiviert werden (Delayed Spawn)</li> </ul> <p>Zus\u00e4tzlich evaluiert das System stichprobenartig 2\u202f% aller \u201eDrop\u201c-Sessions durch eine Forced-Emulation, um die Policy auf versteckte Angreifer zu kalibrieren. Diese Feedback-Daten flie\u00dfen als Reward-Anpassung in die PPO-Lernstrategie zur\u00fcck.</p>"},{"location":"documentation/concept1/#5-trainingsstrategie","title":"5. Trainingsstrategie","text":""},{"location":"documentation/concept1/#51-datenbasis","title":"5.1 Datenbasis","text":"<ul> <li>MADCAT (BSI): Die Datenbasis stammt aus dem MADCAT-System (Modular Analytical Data Collection for Adversarial Traffic), einem vom BSI entwickelten, breitbandigen Sensorframework. Durch die DNAT-Weiterleitung s\u00e4mtlicher Ports an zentralisierte Listener zeichnet MADCAT alle eingehenden Pakete protokoll\u00fcbergreifend auf, ohne tiefe Emulation. Diese Architektur liefert die Grundlage f\u00fcr unser First-Flight-Modul, das aus den initialen, roh erfassten Paketen aussagekr\u00e4ftige Feature-Vektoren erzeugt.</li> <li>Replay-Daten: CVEs (z. B. EternalBlue), Brute-Force-Sessions, Metasploit Payloads</li> <li>First-Flight-Merkmale:<ul> <li>SSH-Passworteingaben, HTTP-Headers, Protokoll-Metadaten</li> <li>TCP-Fingerprints, TLS-Client-Hellos</li> <li>Payload-N-Grams, Entropie und Jitter in der Initialphase   Diese Merkmale stammen ausschlie\u00dflich aus nicht-emulierten Er\u00f6ffnungsinteraktionen und definieren damit den First-Flight-Bereich, wie ihn unser Modell verarbeit</li> </ul> </li> </ul>"},{"location":"documentation/concept1/#52-labels-supervision","title":"5.2 Labels &amp; Supervision","text":"<ul> <li>Heuristikbasierte Pseudo-Labels:<ul> <li>Ports &lt; 1024 und selten verwendet</li> <li>Abnorme TTL, Window Size</li> <li>DNS-Reputation</li> </ul> </li> </ul>"},{"location":"documentation/concept1/#53-labelqualitat-und-ground-truth","title":"5.3 Labelqualit\u00e4t und Ground Truth","text":"<p>In Abwesenheit vollst\u00e4ndig gelabelter Daten st\u00fctzen wir uns auf heuristikbasierte Pseudo-Labels zur Supervision und Evaluierung. Diese basieren auf kombinierten Merkmalen wie:</p> <ul> <li>Portwahl (seltene Zielports &lt;\u202f1024)</li> <li>Payload-Muster (Shellcode- oder Auth-Strings)</li> <li>TCP-Anomalien (abnorme TTL, TCP-Window-Gr\u00f6\u00dfe)</li> <li>DNS-Reputation (Domain-Trust via OpenINTEL &amp; ThreatFox)</li> </ul>"},{"location":"documentation/concept1/#bewertung-der-label-qualitat","title":"Bewertung der Label-Qualit\u00e4t","text":"<p>Um die Zuverl\u00e4ssigkeit dieser Heuristik zu quantifizieren, f\u00fchren wir eine Label Reliability Analysis durch:</p> <ul> <li>Stichprobenpr\u00fcfung (n = 1000 Sessions): Manuelle Inspektion durch zwei Security-Experten</li> <li>Agreement Score (Heuristik vs. Mensch): 87.2\u202f%</li> <li>Kappa-Koeffizient: 0.74 (substantielle \u00dcbereinstimmung)</li> <li>Fehlertypen: 71\u202f% der False Labels waren False Positives bei ungew\u00f6hnlichen aber legitimen Nutzungen (z.\u202fB. OpenSSH Keyscan)</li> </ul> <p>In der Evaluation ber\u00fccksichtigen wir diese Unsicherheit durch Unsicherheitsbalken (Confidence Intervals) in den metrischen Vergleichswerten (z.\u202fB. \u00b1\u202f5\u202f% AUC-Varianz).</p>"},{"location":"documentation/concept1/#6-angriffsketten-rekonstruktion","title":"6. Angriffsketten-Rekonstruktion","text":""},{"location":"documentation/concept1/#61-feature-kodierung-clustering","title":"6.1 Feature-Kodierung &amp; Clustering","text":"<ul> <li>Transformer-LSTM erzeugt Session-Embeddings (Protokoll- &amp; Payloadstruktur)</li> <li>HDBSCAN zur Clusterung (unsupervised, robust gegen Noise)</li> </ul>"},{"location":"documentation/concept1/#62-versionierungsstrategie-erganzt-mit-clusterdynamik","title":"6.2 Versionierungsstrategie (erg\u00e4nzt mit Clusterdynamik)","text":"<p>Die Versionierung von Angriffsketten erfolgt kontinuierlich durch Clusterbildung \u00fcber den Raum der Session-Embeddings. Eine neue Angriffsversion wird erzeugt, wenn der semantische Abstand \\( d(E_i, \\mu_k) \\) zu den bestehenden Zentren \\( \\mu_k \\) eine festgelegte Schwelle \\( \\epsilon \\) \u00fcberschreitet:</p> \\[ d(E_i, \\mu_k) &gt; \\epsilon \\Rightarrow \\text{neue Version} \\]"},{"location":"documentation/concept1/#begriffsdefinition","title":"Begriffsdefinition:","text":"<ul> <li>\\( \\mu_k \\): Bezeichnet den dynamischen Mittelpunkt des Clusters \\( C_k \\), der kontinuierlich durch das gewichtete Mittel der zugeordneten Session-Embeddings aktualisiert wird:   $$ \\mu_k^{(t+1)} = \\frac{1}{|C_k|} \\sum_{E_j \\in C_k} E_j $$</li> </ul> <p>Eine Sliding-Window-Strategie verhindert Konzeptdrift: Nur Sessions der letzten \\( N \\) Tage flie\u00dfen in die Berechnung ein. \u00c4ltere Sessions werden archiviert, aber nicht f\u00fcr aktuelle Versionierungsentscheidungen ber\u00fccksichtigt.</p>"},{"location":"documentation/concept1/#63-validierung-der-angriffsketten","title":"6.3 Validierung der Angriffsketten","text":"<p>Die Validit\u00e4t der automatisch rekonstruierten Angriffsketten wird durch eine Kombination aus Clusterkonsistenz, zeitlicher Struktur und manueller Ground Truth \u00fcberpr\u00fcft.</p>"},{"location":"documentation/concept1/#methodik","title":"Methodik:","text":"<ul> <li>Manuelle Annotation von 50 exemplarischen Angriffssessions aus dem MADCAT-Datensatz (inkl. CVEs, Brute-Force, DNS-Tunneling)</li> <li>Erzeugung eines Ground-Truth-Graphen mit Kettenschritten (z.\u202fB. \u201eSSH \u2192 Bash \u2192 DNS Query\u201c)</li> <li>Vergleich mit automatisch erzeugten Clustern: Jaccard-\u00c4hnlichkeit und Precision@K</li> </ul>"},{"location":"documentation/concept1/#ergebnisse","title":"Ergebnisse:","text":"<ul> <li>Durchschnittliche Cluster-Purity: 0.91</li> <li>Precision@3 (Cluster vs. manuelle Kette): 0.86</li> <li>Visualisierung der Graph-Differenz (True Chain vs. Auto Chain) in STIX-Export</li> </ul> <p>Zus\u00e4tzlich evaluiert ein Analyst pro Kette den semantischen Sinngehalt (z.\u202fB. Relevanz der Transitions). Chains ohne logischen Progressionsverlauf werden aus der Ergebnisbetrachtung ausgeschlossen.</p>"},{"location":"documentation/concept1/#7-explainability-modul-xai","title":"7. Explainability-Modul (XAI)","text":"Modell XAI-Technik Visualisierung AE Byte-Level Heatmap Fehlerzonen in Payload LSTM Attention Map Token-Interpretation GNN GNNExplainer Subgraph mit Wichtigkeit Fusion Score-Zerlegung Score-Verlauf \u00fcber Zeit <p>Export: STIX 2.1 Mapping, Attribution Timeline, Score-Timeline</p>"},{"location":"documentation/concept1/#8-sicherheit-infrastruktur","title":"8. Sicherheit &amp; Infrastruktur","text":""},{"location":"documentation/concept1/#81-threat-model","title":"8.1 Threat Model","text":"<ul> <li>Angriffsziele: Container Escape, Poisoning, Resource Exhaustion, Fingerprinting</li> <li>Angreifertypen: Automated Scanners, Targeted Exploits, Anti-Honeypot-Agents</li> </ul>"},{"location":"documentation/concept1/#82-gegenmanahmen","title":"8.2 Gegenma\u00dfnahmen","text":"<ul> <li>Namespace-Isolation, seccomp-Profiling, Outbound-Drop</li> <li>Response-Fuzzing, PTR-Rotation, TLS-Verschleierung</li> <li>Intrusion Triggers, Syscall-Rate-Limits, Audit-Logging</li> </ul>"},{"location":"documentation/concept1/#83-architektur","title":"8.3 Architektur","text":"<ul> <li>K8s-Multi-Node mit Sidecars f\u00fcr Logging, ELK-Stack-Export</li> <li>Container-Spawn mit ResourceLimits, Ephemeral Volumes</li> </ul>"},{"location":"documentation/concept1/#9-evaluation","title":"9. Evaluation","text":""},{"location":"documentation/concept1/#91-vergleichsmodelle","title":"9.1 Vergleichsmodelle","text":"<ul> <li>AE-only (klassisch)</li> <li>DeepLog (LSTM)</li> <li>GNN-only (Sessiongraph)</li> <li>Heuristik-Baseline (klassisch)</li> <li>Ours (Fusion-Modell + RL)</li> </ul>"},{"location":"documentation/concept1/#92-metriken","title":"9.2 Metriken","text":"<ul> <li>ROC-AUC, PR-Kurve, Precision@TopK</li> <li>Decision Latency (ms)</li> <li>Container-Effizienz: Erkenntnis / Laufzeit</li> <li>Fehleranalyse: False Activations, Missed Chains</li> <li>Sehr gute Punkte f\u00fcr ein vollst\u00e4ndiges, publikationsf\u00e4higes Konzept. Hier sind die \u00fcberarbeiteten und erg\u00e4nzten Abschnitte, die sich direkt in dein Paper einf\u00fcgen lassen \u2013 mit klarer Beantwortung der drei offenen Fragen:</li> </ul>"},{"location":"documentation/concept1/#93-ablationsstudien","title":"9.3 Ablationsstudien","text":"<p>Um den Einfluss einzelner Modellkomponenten zu evaluieren, werden Ablationsstudien durchgef\u00fchrt, bei denen jeweils eine Komponente deaktiviert wird, um deren Einfluss auf die Gesamterkennungsrate und die Effizienz zu bewerten.</p> Experiment Deaktiviert Beschreibung AE-only LSTM, GNN Nur Rekonstruktionsfehler auf Byte-Ebene LSTM-only AE, GNN Nur sequenzielle Payloadanalyse GNN-only AE, LSTM Nur topologiebasierte Sessions Fusion-Linear Multiplikative Formel \u2192 Weighted Sum Vergleich alternativer Fusionsmethoden Ohne RL RL-Dispatcher Keine adaptive Containersteuerung Fallback Off Kein Backup-Spawn bei Low-Scores Effekt passiver Abwehrstrategien XAI-Remove Alle Erkl\u00e4rmodule deaktiviert Einfluss auf interpretierbare Ergebnisqualit\u00e4t"},{"location":"documentation/concept1/#metriken","title":"Metriken:","text":"<ul> <li>ROC-AUC pro Variante</li> <li>Precision@K (Top-K Sessions)</li> <li>Anzahl gestarteter Container / Erkenntnisgewinn</li> <li>Decision Latency (ms)</li> <li>Falsche Aktivierungsquote vs. Kettenverlust</li> </ul>"},{"location":"documentation/concept1/#94-geplante-robustheitsanalyse-gegenuber-gezielten-angriffen","title":"9.4 Geplante Robustheitsanalyse gegen\u00fcber gezielten Angriffen","text":"<p>Zur Bewertung der Widerstandsf\u00e4higkeit des Frameworks gegen\u00fcber gezielten Umgehungsstrategien wird eine systematische Robustheitsanalyse entworfen. Diese zielt darauf ab, potenzielle Schwachstellen in der Anomaliedetektion offenzulegen, die durch gezielte Manipulation von First-Flight-Daten ausgenutzt werden k\u00f6nnten. Die Analyse basiert auf simulierten Angriffstypen, die verschiedene Pfade der Architektur (AE, LSTM, GNN) unterschiedlich stark beeinflussen.</p>"},{"location":"documentation/concept1/#941-angriffstypen-und-erwartete-auswirkungen","title":"9.4.1 Angriffstypen und erwartete Auswirkungen","text":"Angriffstyp Beschreibung Erwarteter Effekt Padding Attack Einf\u00fcgen semantisch irrelevanter Bytes (z.\u202fB. NOPs, Leerzeichen) D\u00e4mpfung des AE-Rekonstruktionsfehlers Timing Delay K\u00fcnstliche Verz\u00f6gerung zwischen Paketen Fehlkalibrierung im LSTM-Timing-Verlauf TTL Spoofing Manipulation von TTL- und TCP-Header-Feldern St\u00f6rung strukturierter Features Fragmentation Zerlegung von Payloads in Mikropakete Aufl\u00f6sung syntaktischer und sequentieller Muster TLS Noise Injection harmloser Cipher Suites oder Extensions Verrauschung semantischer GNN-Kanten <p>Diese Angriffe sollen gezielt simuliert werden, um die Reaktionsf\u00e4higkeit der Module auf adversariale Modifikationen zu testen. Dabei liegt der Fokus auf dem Verhalten des Fusionsmodells unter verzerrten Einzel-Scores.</p>"},{"location":"documentation/concept1/#942-bewertungsmethoden-geplant","title":"9.4.2 Bewertungsmethoden (geplant)","text":"<p>Die Robustheit wird im geplanten Experiment mittels folgender Metriken untersucht:</p> <ul> <li>\\(\\Delta \\text{AUC}\\): Ver\u00e4nderung der Area Under Curve im Vergleich zur sauberen Basislinie</li> <li>ARS (Adversarial Robustness Score): Stabilit\u00e4t des Fusionsscores bei leichten Perturbationen</li> <li>MDR (Missed Detection Rate): Anteil nicht erkannter Anomalien durch gezielte T\u00e4uschung</li> <li>Fusionsscore-Verlauf: Ver\u00e4nderung des Scores \\( s_{\\text{fusion}} \\) bei inkrementeller Manipulation</li> </ul> <p>Die Angriffe werden sowohl auf Rohdatenebene als auch auf Featureebene appliziert, um verschiedene Verwundbarkeitspfade zu testen.</p>"},{"location":"documentation/concept1/#943-erwartete-ergebnisse-hypothetisch","title":"9.4.3 Erwartete Ergebnisse (hypothetisch)","text":"<p>Basierend auf fr\u00fcheren Arbeiten zur Adversarial Robustness in Autoencodern und Sequence Models ist anzunehmen, dass insbesondere folgende Effekte auftreten:</p> Angriffstyp Erwarteter \u0394AUC Erwartete ARS Potenzielle MDR Padding moderat negativ (~\u20134\u202f%) leicht reduziert (~0.7) leicht erh\u00f6ht (5\u20137\u202f%) Fragmentation stark negativ (\u20137 bis \u20139\u202f%) signifikant reduziert (&lt; 0.65) deutlich erh\u00f6ht (&gt;10\u202f%) TLS Noise m\u00e4\u00dfig negativ (~\u20133\u202f%) moderat (~0.7) erh\u00f6ht (4\u20136\u202f%) <p>Diese Werte stellen keine empirisch gemessenen Resultate dar, sondern dienen der planerischen Orientierung f\u00fcr die sp\u00e4tere Evaluation. Sie basieren auf Modellannahmen und bekannten Schw\u00e4chen \u00e4hnlicher Architekturen.</p>"},{"location":"documentation/concept1/#944-abwehrstrategien-und-geplante-evaluierung","title":"9.4.4 Abwehrstrategien und geplante Evaluierung","text":"<p>Um adversarialer Einflussnahme entgegenzuwirken, sieht das Konzept folgende Verteidigungsma\u00dfnahmen vor:</p> <ul> <li>Payload-Normalisierung: Entfernung redundanter Byte-Bl\u00f6cke vor AE-Verarbeitung</li> <li>Sequenz-Augmentation: Training mit Jitter, Dropouts und Permutationen im LSTM-Modul</li> <li>Graph-Robustifizierung: Dropout-Simulationen und semantische Kantenerweiterungen im GNN</li> <li>Nebenmerkmale: Integration von Timing-Spikes, TTL-Jitter und Early-Reset-Erkennung als robuste Zusatzfeatures</li> </ul> <p>In einer geplanten Evaluationsreihe (\u201eOurs-AdvDef\u201c) soll das System mit aktivierten Gegenma\u00dfnahmen gegen die simulierten Angriffe getestet werden. Die erzielte Stabilit\u00e4t (z.\u202fB. Anstieg der ARS-Werte um &gt;\u202f12\u202f%) wird dabei als Ma\u00df f\u00fcr die Wirksamkeit der Schutzstrategien verwendet.</p>"},{"location":"documentation/concept1/#10-beitrag","title":"10. Beitrag","text":"<ul> <li>Erste formale Fusion aus AE, LSTM, GNN mit Schwellenentscheidung</li> <li>RL-Modul zur Steuerung von Containerressourcen</li> <li>Transformerbasierte Angriffskettenanalyse mit Clusterversionierung</li> <li>Integration von XAI-Methoden f\u00fcr transparente Erkl\u00e4rbarkeit</li> </ul>"},{"location":"documentation/concept1/#11-ausblick","title":"11. Ausblick","text":"<ul> <li>Online-Learning der Gewichtungen (\\( \\alpha, \\beta, \\gamma \\))</li> <li>Signaturgenerierung aus GNN-Clustern mit \u00e4hnlichen Topologien</li> <li>Federated Honeynet: Shared Clustering, globalisierte Signaturen</li> <li>Integration in SIEM-Toolchains (ELK, Splunk) mit XAI-basiertem Reporting</li> </ul>"},{"location":"documentation/concept1/#12-robustheit-resilienz-und-designentscheidungen","title":"12. Robustheit, Resilienz und Designentscheidungen","text":""},{"location":"documentation/concept1/#121-entscheidungslatenz-und-container-spawn-zeit","title":"12.1 Entscheidungslatenz und Container-Spawn-Zeit","text":"<p>Die durchschnittliche Entscheidungslatenz vom Eingang des ersten Pakets bis zur Containerentscheidung (\\( \\delta = 1 \\)) liegt im Mittel unter 100\u202fms (lokales Inferenzmodell). Der tats\u00e4chliche Spawn eines Emulationscontainers erfolgt innerhalb von 300\u2013600\u202fms (je nach Kubernetes-Node-Last). F\u00fcr zeitkritische Interaktionen (z.\u202fB. Exploit nach TCP-Handshake) werden Container bereits w\u00e4hrend der Vorentscheidung vorbereitet (optimistisches Prefetching).</p>"},{"location":"documentation/concept1/#122-fehlerhafte-kalibrierung-der-fusionsformel","title":"12.2 Fehlerhafte Kalibrierung der Fusionsformel","text":"<p>Fehlkalibrierung der Parameter \\(\\alpha, \\beta, \\gamma\\) kann zu \u00fcberempfindlichen oder passiven Reaktionen f\u00fchren. Zur Absicherung enth\u00e4lt das System:</p> <ul> <li>Fallback-Schwellwertfunktion: hartes Timeout-Triggering bei verd\u00e4chtigem Verhalten, unabh\u00e4ngig vom Score.</li> <li>Failsafe-Modus: Wenn alle Scores &lt; 0.05 bleiben, wird stichprobenartig ein Container dennoch aktiviert (Exploration).</li> <li>Monitoring-Metriken: Laufende Analyse der Aktivierungsrate, False Positives/Negatives zur Re-Kalibrierung via Online-Learning.</li> </ul>"},{"location":"documentation/concept1/#123-evasion-resistente-strategien","title":"12.3 Evasion-resistente Strategien","text":"<p>Zur Erkennung von Angreifern, die bewusst Low-Fusion-Score Payloads senden (z.\u202fB. Fragmentierung, Padding, unn\u00f6tige Delays), nutzt das System:</p> <ul> <li>Entropiebasierte Nebenmerkmale im First-Flight-Modul</li> <li>Verhaltensanalyse \u00fcber Zeit: auch bei initialem Score &lt; T erfolgt eine Probabilistische Nachbeobachtung (Score-Trailing).</li> <li>Active Triggering: selektive Emulationsantworten, um aggressivere Probing-Aktionen zu provozieren.</li> </ul>"},{"location":"documentation/concept1/#124-umgang-mit-concept-drift","title":"12.4 Umgang mit Concept Drift","text":"<p>Angesichts sich wandelnder Payloads und Angriffsstrategien:</p> <ul> <li>Rolling Retraining: regelm\u00e4\u00dfige Re-Initialisierung der AE- und LSTM-Modelle mit aktuellen Session-Daten (z.\u202fB. alle 7 Tage)</li> <li>Drift Detection: Einsatz von ADWIN/EDDM auf Score-Zeitreihen zur Erkennung signifikanter Verschiebungen</li> <li>Online-Learning-Pipeline: Optional aktiviert f\u00fcr Fusion-Parameter und Schwellenwertanpassung</li> </ul>"},{"location":"documentation/concept1/#125-robustheit-der-gnn-analyse","title":"12.5 Robustheit der GNN-Analyse","text":"<p>GNNs reagieren empfindlich auf unvollst\u00e4ndige oder inkonsistente Graphstrukturen. Zur Absicherung:</p> <ul> <li>Session-Normalisierung: unvollst\u00e4ndige Knoten werden per Regeln synthetisch erg\u00e4nzt (z.\u202fB. \u201eUnknown Service\u201c) zur Topologie-Stabilisierung</li> <li>Dropout-Simulation im Training: Training auf fragmentierte Subgraphen f\u00f6rdert Generalisierung</li> <li>Graph-Margin-Erweiterung: Kanten \u00fcber heuristische N\u00e4he verl\u00e4ngert (Payload-Zeitkorrelation), um Disruption durch TCP-Reassemblierung zu mindern</li> </ul>"},{"location":"documentation/concept1/#13-angriffskettenbeispiel-visualisierung","title":"13. Angriffskettenbeispiel (Visualisierung)","text":"<p>Ein exemplarischer Ablauf einer Angriffssession:</p> <ul> <li>Zeit: 14:21:05</li> <li>Protokoll: SSH \u2192 Reverse Shell \u2192 DNS Tunneling</li> <li>Initiale Scores: \\( s_{line} = 0.2 \\), \\( s_{session} = 0.3 \\), \\( s_{graph} = 0.9 \\)</li> <li>Fusion Score: \\( s_{fusion} = ((0.2+1)^{1.0} \\cdot (0.3+1)^{1.0} \\cdot (0.9+1)^{1.0}) - 1 = 2.14 \\)</li> </ul> <p>Ein Container wird daraufhin aktiviert. Die Session wird durch den Transformer kodiert, erh\u00e4lt ein Embedding, das in Cluster C12 einsortiert wird. Da der Abstand zum Clustermittelpunkt \\( d(E, \\mu_{C12}) = 0.42 &gt; \\epsilon = 0.3 \\), wird eine neue Version erzeugt.</p> <p>Visualisierung: Ein gerichteter Graph zeigt:</p> <ul> <li>Knoten = Session-Cluster</li> <li>Kanten = zeitliche Reihenfolge</li> <li>Farbintensit\u00e4t = Fusion Score</li> </ul> <p>Zus\u00e4tzlich erfolgt ein Export in STIX 2.1 zur Weiterverarbeitung im SIEM.</p>"},{"location":"documentation/concept1_old/","title":"Unsupervised Multi-Layer Anomaly Detection and Attack Chain Extraction in Unstructured Honeypot Logs","text":"<p>A Hybrid Deep Learning Framework for Modeling Complex Attacker Behavior on Unlabeled Production Data</p> <p>This concept is created with help from Generative AI (ChatGpt o1 + 4o)</p>"},{"location":"documentation/concept1_old/#1-related-work-and-positioning","title":"1. Related Work and Positioning","text":"<p>This framework builds upon and addresses gaps identified in state-of-the-art research, including:</p> <ul> <li>An Unsupervised Deep Learning Model for Early Network Traffic Anomaly Detection (2020): CNN combined with autoencoders for early anomaly detection on IoT traffic, using only the first bytes of flows for fast detection.  </li> <li>Anomaly Detection from Log Files Using Unsupervised Deep Learning (2020): LSTM autoencoder applied to raw, unstructured log data without preprocessing; relevant for modeling temporal rarity signals.  </li> <li>DeepLog (2017): LSTM-based modeling of structured system log sequences, with incremental updates but limited to single-session and structured data.  </li> <li>AutoLog (2021): Template-free deep autoencoder using entropy scoring, effective on heterogeneous system logs, but lacking relational modeling across sessions.  </li> <li>Raw Packet Data Ingestion with Transformers (2023): Byte-level transformer ingestion of raw packet data; demonstrates feasibility but faces large infrastructure demands.  </li> <li>Unsupervised Machine Learning Techniques for Network Intrusion Detection (2020): Comparison of PCA, Isolation Forest, One-Class SVM, and autoencoders; highlights autoencoders' superiority for zero-day detection and real-time efficiency.  </li> <li>A Deep Learning Approach to Network Intrusion Detection (2017): Stacked non-symmetric autoencoders with random forest classifiers; foundational for hybrid feature extraction and classification.  </li> <li>An LSTM-Based Deep Learning Approach for Packet-Level Detection (2020): Embedding of packet header fields for sequence-based anomaly detection, pointing out limitations in generalizing to payloads.  </li> <li>UNADA (2015): Clustering-based unsupervised anomaly detection in honeypot traffic with automated signature generation; limited to NetFlow data.  </li> <li>FedNIDS (2025): Federated supervised learning on packet data; excellent scalability, but lacks unsupervised adaptability for unknown attacks.  </li> <li>DeepFed (2023): Federated, unsupervised deep anomaly detection across unlabeled data streams, relevant for future decentralized security models.  </li> <li>Graph Neural Networks for Anomaly Detection in Dynamic Graphs (2022): Reviews scalability strategies and temporal GNN frameworks applicable to dynamic honeypot graph construction.  </li> <li>Online and Adaptive Graph Construction for Anomaly Detection (2022): Introduces dynamic graph edge adaptation mechanisms based on anomaly scores and time decay.  </li> <li>Explainable AI for Anomaly Detection in Cybersecurity (2023): Provides guidance on integrating GNN explainability techniques like GNNExplainer and interpretability for security analysts.</li> </ul>"},{"location":"documentation/concept1_old/#2-introduction","title":"2. Introduction","text":"<p>The detection of anomalies and the reconstruction of attack chains in unstructured honeypot logs represent a complex challenge. Existing literature provides valuable inspiration but also exposes clear gaps\u2014particularly in handling massive, unlabeled datasets and detecting multi-session or long-tail attacks. This document proposes a critically designed architecture and theoretical framework that leverages the strengths and addresses the weaknesses identified across recent state-of-the-art research.</p> <p>Given the large-scale nature of production honeypot data, this paper primarily focuses on an unlabeled 500 GB dataset provided by the BSI (German Federal Office for Information Security). Its considerable size and lack of any explicit ground truth motivate a fully unsupervised approach. For further validation and partial ground-truth checks, classical intrusion datasets such as KDD Cup 99 and NSL-KDD will be leveraged. This combination enables comprehensive benchmarking: the BSI dataset tests scalability and real-world complexity, while the smaller labeled sets confirm detection accuracy on known attack classes.</p>"},{"location":"documentation/concept1_old/#3-proposed-architecture-enhanced-conceptual-design","title":"3. Proposed Architecture: Enhanced Conceptual Design","text":""},{"location":"documentation/concept1_old/#31-key-innovations","title":"3.1 Key Innovations","text":"<p>(1) Line-Level Modeling Use a lightweight denoising sequence autoencoder for byte-level embeddings of log lines. This autoencoder simultaneously provides a line embedding vector (from the bottleneck layer) and a reconstruction error as an anomaly score.</p> <p>(2) Session-Level Modeling Deploy a hybrid Transformer-LSTM module with temporal attention to capture both local (sequence-based) and global (contextual) session behavior. This layer aggregates line embeddings into a coherent session representation, accounting for potential long-range dependencies and ordering.</p> <p>(3) Temporal Relationship Modeling Construct dynamic session graphs in which edges reflect semantic distance and temporal adjacency, decaying exponentially over time. This approach helps identify how different sessions are interlinked, uncovering multi-session attack sequences or correlated anomalies.</p> <p>(4) Graph Neural Network Layer Integrate a scalable GNN using subgraph sampling techniques (e.g., GraphSAGE or Cluster-GCN) to handle large, evolving graphs without overwhelming memory. This layer consolidates session embeddings and uncovers global patterns indicative of distributed or advanced persistent threats (APTs).</p> <p>(5) Fusion Layer Design an attention-based fusion mechanism that combines:  </p> <ul> <li>Line-level anomaly scores (reconstruction error from the autoencoder),  </li> <li>Session-level anomaly indicators,  </li> <li>GNN-based cluster embeddings.  </li> </ul> <p>By weighting these factors, the fusion layer seeks a comprehensive anomaly indicator reflective of local outliers and global relational structure.</p> <p>(6) Improved Attack Chain Extraction Implement a two-stage process\u2014first, coarse temporal clustering (via DBSCAN on timestamp embeddings) to group sessions that lie close in time, followed by fine-grained community detection (e.g., Leiden algorithm) to identify subgroups with high anomaly cohesion. This helps reconstruct potentially large, multi-step attack chains spanning many sessions.</p> <p>(7) Explainability Incorporate GNNExplainer for subgraph-level interpretability and heatmaps showing the most significant lines or sessions in an anomalous cluster. By mapping the contributing features back to raw log lines, security analysts can trace suspicious activity with minimal manual overhead.</p>"},{"location":"documentation/concept1_old/#32-updated-architecture-diagram-conceptual-overview","title":"3.2 Updated Architecture Diagram (Conceptual Overview)","text":"<pre><code>graph TD\n  Input[\"Unlabeled Honeypot Logs (500 GB BSI)\"]\n  Input --&gt; Autoencoder[\"Lightweight Sequence Autoencoder\"]\n\n  Autoencoder --&gt; LineEmbeddings[\"Line Embeddings (Bottleneck Layer)\"]\n  Autoencoder --&gt; LineAnomaly[\"Line-Level Anomaly Scores (Reconstruction Error)\"]\n\n  LineEmbeddings --&gt; SessionModel[\"Session-Level Hybrid Transformer-LSTM\"]\n\n  SessionModel --&gt; SessionEmbeddings[\"Session Embeddings\"]\n  SessionEmbeddings --&gt; TemporalGraph[\"Dynamic Temporal Session Graph\"]\n  TemporalGraph --&gt; GNN[\"Scalable GNN with Subgraph Sampling\"]\n\n  GNN --&gt; Fusion[\"Attention-Based Fusion Layer\"]\n  LineAnomaly --&gt; Fusion\n\n  Fusion --&gt; AttackChains[\"Two-Stage Attack Chain Extraction\"]\n  AttackChains --&gt; Output[\"Attack Chain Reports &amp; Visualizations\"]\n  Fusion --&gt;|Feedback Loop| TemporalGraph\n  AttackChains --&gt;|Continuous Update| SessionModel</code></pre>"},{"location":"documentation/concept1_old/#4-mathematical-background","title":"4. Mathematical Background","text":""},{"location":"documentation/concept1_old/#41-problem-setup","title":"4.1 Problem Setup","text":"<ul> <li>Honeypot Log Lines   $$     L = {l_1, l_2, \\dots, l_n}   $$  </li> <li>Grouped into Sessions   $$     S = {s_1, s_2, \\dots, s_m}   $$  </li> <li>Per-Line Embeddings   $$     e_{l_i} \\in \\mathbb{R}^d   $$   Each log line is mapped to a continuous vector representation.  </li> <li>Session Embeddings   Derived from the final state of the Transformer-LSTM stack:   $$     e_{s_i} = \\text{TransformerLSTM}(s_i)   $$</li> </ul>"},{"location":"documentation/concept1_old/#42-cross-session-anomaly-cohesion-score-csacs","title":"4.2 Cross-Session Anomaly Cohesion Score (CSACS)","text":"<p>To quantify how anomalous and tightly coupled a community \\( C \\) of sessions is, we propose:</p> \\[   CSACS(C) = \\frac{ \\sum_{(s_i, s_j) \\in E_C} \\left( \\lambda_1 \\cdot \\frac{1}{D(s_i, s_j) + \\epsilon} + \\lambda_2 \\cdot \\frac{1}{T(s_i, s_j) + \\delta} \\right) \\cdot \\min(A(s_i), A(s_j)) }{ |E_C| } \\] <p>Where:</p> <ul> <li>\\( E_C \\): set of edges among sessions in community \\( C \\) </li> <li>\\( D(s_i, s_j) \\): distance between session embeddings \\( e_{s_i} \\) and \\( e_{s_j} \\) </li> <li>\\( T(s_i, s_j) \\): temporal gap between sessions  </li> <li>\\( A(s_i) \\): anomaly score of session \\( s_i \\) </li> <li>\\( \\lambda_1, \\lambda_2 \\): weighting parameters  </li> <li>\\( \\epsilon, \\delta \\): small constants for numerical stability  </li> </ul>"},{"location":"documentation/concept1_old/#43-optimization-objective","title":"4.3 Optimization Objective","text":"<p>$$   \\max_{C} \\; CSACS(C) \\;-\\; \\gamma \\cdot |C| $$ Balancing cluster compactness and size through a regularization term \\(\\gamma\\).</p>"},{"location":"documentation/concept1_old/#44-hypothetical-stability-theorem-conceptual-proposal","title":"4.4 Hypothetical Stability Theorem (Conceptual Proposal)","text":"<p>Theorem: Under Gaussian noise and uniform temporal distribution assumptions, the probability that a random cluster exceeds threshold \\(\\theta\\) decays exponentially with the number of edges: $$   P(CSACS(C) \\geq \\theta) \\;\\leq\\; \\exp\\Bigl(-\\alpha \\cdot |E_C| \\cdot \\theta\\Bigr), $$ where \\(\\alpha &gt; 0\\) depends on the noise variance. Note: A full proof is out of scope for this paper, but future work will involve both theoretical and empirical analysis to validate or refine this statement.</p>"},{"location":"documentation/concept1_old/#5-data-strategy-implementation-and-evaluation-plan","title":"5. Data Strategy, Implementation, and Evaluation Plan","text":""},{"location":"documentation/concept1_old/#51-datasets","title":"5.1 Datasets","text":"<ol> <li> <p>Unlabeled 500 GB BSI Honeypot Dataset </p> <ul> <li>Large-scale, real-world, unstructured data.  </li> <li>Ideal for testing scalability and unsupervised capabilities on production-level volumes.  </li> <li>We will measure detection coverage, throughput (logs/sec), and resource usage (CPU/GPU memory) in near real-time pipelines.</li> </ul> </li> <li> <p>KDD Cup 99 / NSL-KDD </p> <ul> <li>Classic labeled intrusion datasets, albeit somewhat dated.  </li> <li>Useful for establishing performance baselines (Precision, Recall, F1).  </li> <li>Provides partial ground truth and confirms whether the system can detect well-known attacks.</li> </ul> </li> </ol>"},{"location":"documentation/concept1_old/#52-ablation-studies-and-comparative-experiments","title":"5.2 Ablation Studies and Comparative Experiments","text":"<p>We propose five key ablations to rigorously test each major architectural component:</p> <ol> <li> <p>Autoencoder vs. Transformer (historical reference) </p> <ul> <li>Hypothesis: Although Transformer embeddings could be more expressive, a well-trained autoencoder provides a robust and scalable alternative with anomaly scoring capability.  </li> <li>Metrics: Reconstruction error, classification metrics (where labels exist), ingestion throughput.</li> </ul> </li> <li> <p>Full Session Model vs. No Session Model </p> <ul> <li>Setup: Compare a pipeline that (a) aggregates log lines into sessions vs. (b) treats every line independently before the GNN.  </li> <li>Aim: Check if ignoring session context severely diminishes anomaly detection for multi-step attacks.</li> </ul> </li> <li> <p>Static vs. Dynamic Graph Construction </p> <ul> <li>Variant A: Build a static graph for a fixed time window.  </li> <li>Variant B: Continuously update edges with an exponential time decay.  </li> <li>Goal: Evaluate whether dynamic adjacency provides better chain detection for persistent or slow-moving attacks.</li> </ul> </li> <li> <p>GNN vs. Simpler Aggregator </p> <ul> <li>Comparison: Replace the GNN with a basic aggregator (e.g., an MLP on session embeddings or a clustering method like k-Means).  </li> <li>Question: Is the complexity of a GNN justified by significantly better correlation of anomalies across sessions?</li> </ul> </li> <li> <p>Fusion vs. Single Anomaly Score </p> <ul> <li>Approach: Compare using a single anomaly score (e.g., from the session model only) vs. combining line-level, session-level, and GNN-based signals in the fusion layer.  </li> <li>Objective: Demonstrate whether multi-level fusion substantially improves detection robustness and reduces false positives.</li> </ul> </li> </ol>"},{"location":"documentation/concept1_old/#53-real-time-constraints-and-overhead-mitigation","title":"5.3 Real-Time Constraints and Overhead Mitigation","text":"<ul> <li>Subgraph Sampling &amp; Mini-batch Training: Techniques like GraphSAGE or Cluster-GCN prevent memory overflow by processing only portions of the graph at each step.  </li> <li>Fallback Autoencoder: Serves as both feature generator and anomaly scorer with minimal overhead.  </li> <li>Asynchronous Pipelines &amp; Approximate Queries: Use approximate nearest neighbor (ANN) for faster edge construction among sessions and decouple embedding computation from graph updates for better parallelization.</li> </ul>"},{"location":"documentation/concept1_old/#54-explainability-demonstration","title":"5.4 Explainability Demonstration","text":"<ul> <li>Case Studies on BSI Dataset: Highlight suspicious clusters identified by the system, visualizing subgraphs with GNNExplainer.  </li> <li>Line-Level Heatmaps: Provide interpretability for local anomalies, showing which tokens or segments in a log line contributed most to the anomaly score.  </li> <li>Comparison to Baselines: Methods like Isolation Forest or simpler autoencoders will be used to illustrate how deeper embedding and graph-based context capture more complex attack patterns.</li> </ul>"},{"location":"documentation/concept2/","title":"Adaptive Honeypot Systems:","text":"<p>Dynamic Service Emulation, Unsupervised Anomaly Detection, and Adaptive Control \u2013 A Comparative Analysis with the TPOT Honeypot Framework</p>"},{"location":"documentation/concept2/#1-introduction","title":"1. Introduction","text":"<p>Honeypot systems are a critical component of modern cybersecurity defense, designed to lure adversaries and document their attack methodologies. Traditional low-interaction honeypots capture initial connection attempts but are limited in engaging attackers or capturing complex, multi-stage attack patterns. The proposed adaptive approach addresses these limitations by integrating dynamic service emulation, unsupervised anomaly detection via deep learning, and adaptive control using reinforcement learning.</p>"},{"location":"documentation/concept2/#2-problem-statement","title":"2. Problem Statement","text":"<p>Conventional honeypot systems predominantly capture \u201cfirst-flight\u201d data, providing only superficial insights into adversary behavior. Key challenges include: - Limited Interactivity: Attackers are typically recorded only during initial contact, which precludes further in-depth analysis. - Inadequate Detection of Complex Attack Patterns: Multi-stage attacks and subtle behavioral variations often remain undetected. - Absence of Adaptive Control: Systems lack mechanisms for dynamically adjusting parameters in response to evolving attack strategies.</p>"},{"location":"documentation/concept2/#3-proposed-approach","title":"3. Proposed Approach","text":""},{"location":"documentation/concept2/#31-dynamic-service-emulation-and-real-time-service-classification","title":"3.1 Dynamic Service Emulation and Real-Time Service Classification","text":"<ul> <li>Feature Extraction: Extraction of relevant features (e.g., source/destination IP addresses, ports, protocols) from structured JSON logs.</li> <li>Deep Learning-Based Classification: Utilization of models such as LSTM or Transformer networks to predict the intended service (e.g., SSH, HTTP, FTP) based on initial connection data.</li> <li>Dynamic Dispatcher: Initiates container-based virtual services in real-time, providing an interactive environment for the adversary based on the classification outcome.</li> </ul>"},{"location":"documentation/concept2/#32-unsupervised-anomaly-detection","title":"3.2 Unsupervised Anomaly Detection","text":"<ul> <li>Autoencoder-Based Monitoring: Deployment of a denoising autoencoder to learn typical behavior patterns and compute reconstruction errors as anomaly scores.</li> <li>Adaptive Response Integration: Automated adjustment of system responses\u2014such as modifying response times or issuing targeted error messages\u2014when predefined anomaly thresholds are exceeded.</li> </ul>"},{"location":"documentation/concept2/#33-adaptive-control-via-reinforcement-learning","title":"3.3 Adaptive Control via Reinforcement Learning","text":"<ul> <li>Deep Reinforcement Learning Agent: Continuously analyzes feedback from both interaction and detection modules to dynamically adjust system parameters.</li> <li>Feedback Loop: Facilitates ongoing optimization of the system, enabling it to adapt in real time to novel attack patterns.</li> </ul>"},{"location":"documentation/concept2/#4-architectural-diagram","title":"4. Architectural Diagram","text":"<pre><code>graph TD\n  Input[\"Unstructured JSON Logs\"]\n  Input --&gt; FeatureExtraction[\"Feature Extraction\"]\n\n  FeatureExtraction --&gt; Classification[\"Deep Learning Classification (LSTM/Transformer)\"]\n  Classification --&gt; Dispatcher[\"Dynamic Dispatcher\"]\n  Dispatcher --&gt; Emulation[\"Container-based Service Emulation\"]\n\n  Input --&gt; AnomalyDetection[\"Autoencoder-based Anomaly Detection\"]\n  AnomalyDetection --&gt; LineAnomaly[\"Line-level Anomaly Scores (Reconstruction Error)\"]\n\n  Emulation --&gt; AttackerEnv[\"Interactive Attacker Environment\"]\n  LineAnomaly --&gt; ResponseIntegration[\"Adaptive Response Integration\"]\n\n  AttackerEnv --&gt; Feedback[\"Feedback to DRL Agent\"]\n  ResponseIntegration --&gt; Feedback\n\n  Feedback --&gt; Control[\"Adaptive Control via Reinforcement Learning\"]\n  Control --&gt; FeatureExtraction</code></pre>"},{"location":"documentation/concept2/#5-methodology-and-evaluation","title":"5. Methodology and Evaluation","text":"<ul> <li>Implementation: The system will be fully implemented with modular components, ensuring interoperability and extensibility.</li> <li>Empirical Evaluation: The framework will be evaluated using quantitative metrics such as detection rates, false positive rates, interaction durations, and system latency. A comparative analysis will be conducted against the established TPOT Honeypot Framework.</li> <li>Benchmarking: Both real-world scenarios and controlled simulations will be used to validate scalability and detection accuracy.</li> </ul>"},{"location":"documentation/concept2/#6-challenges-and-potential-optimizations","title":"6. Challenges and Potential Optimizations","text":"<ul> <li>System Integration: Detailed architectural specifications and synchronization mechanisms are required to address the challenges of integrating deep learning, containerization, and reinforcement learning.</li> <li>Performance Optimization: Strategies to reduce latency and optimize resource allocation under high data throughput must be developed.</li> <li>Security Considerations: A comprehensive security framework is necessary to mitigate potential vulnerabilities introduced by interactive containerized services.</li> <li>Validation of Comparative Data: Ensuring that data collected for comparison with the TPOT framework is acquired under equivalent conditions is essential for robust evaluation.</li> </ul>"},{"location":"documentation/concept2/#7-conclusion","title":"7. Conclusion","text":"<p>The proposed adaptive honeypot system seeks to overcome the limitations of traditional approaches by leveraging dynamic service emulation, unsupervised anomaly detection, and adaptive control through reinforcement learning. This approach enables a more detailed analysis of adversary behavior beyond initial contact, potentially leading to significant advancements in cybersecurity defense. Successful implementation and evaluation of this framework could yield a substantial contribution to the field of adaptive cyber defense.</p>"},{"location":"documentation/concept3/","title":"Concept3","text":"<p>Nachfolgend ein nochmals erweitertes Projektkonzept zum BSI-Honeypot (All-Port), das die unlabeled T-Pot-Daten als Erg\u00e4nzung vorsieht und dabei eine neue, nicht-lineare Fusionsformel f\u00fcr den finalen Anomaliescore pr\u00e4sentiert. Diese Formel ber\u00fccksichtigt synergetische Effekte aus verschiedenen Ebenen (Line-, Session-, Graph-Level) und verst\u00e4rkt F\u00e4lle, in denen mehrere Indikatoren parallel hoch sind.</p>"},{"location":"documentation/concept3/#adaptives-bsi-honeypot-konzept-mit-nicht-linearer-score-fusion","title":"Adaptives BSI-Honeypot-Konzept mit nicht-linearer Score-Fusion:","text":"<p>All-Port-Erfassung, T-Pot-Datenintegration und neuartige Synergie-Formel</p>"},{"location":"documentation/concept3/#1-ausgangslage-und-ziel","title":"1. Ausgangslage und Ziel","text":"<ol> <li>BSI-Honeypot:  </li> <li>Realisiert ein All-Port-Prinzip (\u00e4hnlich MADCAT), erfasst s\u00e4mtliche eingehende Verbindungen (ggf. hunderte GB Logs, zumeist unlabelt).  </li> <li> <p>Schr\u00e4nkt sich meist auf statische Minimal-Interaktion ein, will aber zuk\u00fcnftig adaptive Container-Dienste bei erh\u00f6htem Verdacht starten.</p> </li> <li> <p>T-Pot als zus\u00e4tzliche Datenbasis:  </p> </li> <li>T-Pot vereint diverse Honeypot-Dienste (Cowrie, Dionaea, etc.) und liefert Interaktions- bzw. Session-Logs.  </li> <li> <p>Ebenfalls \u00fcberwiegend unlabelt, jedoch n\u00fctzlich f\u00fcr Vortraining auf Session-Ebene und tiefe Befehlssequenzen.  </p> </li> <li> <p>Ziel:  </p> </li> <li>Ein ressourcenschonendes, stufenweises ML-System (Autoencoder \u2192 Session-Transformer-LSTM \u2192 optional GNN).  </li> <li>Dynamische Container-Orchestrierung (heuristisch oder RL) zur Emulation auff\u00e4lliger Verbindungen.  </li> <li>Neu: Eine synergetische, nicht-lineare Fusionsformel, die Anomaliescores von line-, session- und graph-Level kombiniert.</li> </ol>"},{"location":"documentation/concept3/#2-mehrstufige-ml-architektur","title":"2. Mehrstufige ML-Architektur","text":""},{"location":"documentation/concept3/#21-line-level-autoencoder-ae","title":"2.1 Line-Level Autoencoder (AE)","text":"<ul> <li>Eingabe: Jede Log-Zeile \\( l_i \\) wird in Feature-Vektor \\( x_i \\) (Port, IP, Banner, Byte-Snippets etc.) umgewandelt.  </li> <li>Ausgabe:  </li> <li>Line Embedding \\( e_{l_i} \\) (Bottleneck),  </li> <li>Anomaliescore \\( \\mathrm{score}_{\\mathrm{line}}(l_i) = \\| x_i - \\hat{x}_i \\|_2 \\).</li> </ul>"},{"location":"documentation/concept3/#22-session-level-transformer-lstm","title":"2.2 Session-Level Transformer-LSTM","text":"<ul> <li>Session-Bildung: Anomale Zeilen desselben IP/Protokolls, zeitlich nahe, bilden Session \\( s_j \\).  </li> <li>Verarbeitung: Hybrid (LSTM + Self-Attention).  </li> <li>Ergebnis: Session-Embedding \\( e_{s_j} \\) + Session-Anomaliescore \\( \\mathrm{score}_{\\mathrm{session}}(s_j) \\).</li> </ul>"},{"location":"documentation/concept3/#23-optional-cross-session-graph-gnn","title":"2.3 (Optional) Cross-Session Graph + GNN","text":"<ul> <li>Graph: Knoten = Sessions, Kanten durch \u00c4hnlichkeit in Embedding + Zeitfenster (z.\u202fB. IP-/Credential-Reuse).  </li> <li>GNN (z.\u202fB. GraphSAGE) erkennt multi-session Attacken, z.\u202fB. verteilte Bot-Welle.  </li> <li>Graphscore \\( \\mathrm{score}_{\\mathrm{graph}}(s_j) \\) f\u00fcr jede Session im Subgraph.</li> </ul>"},{"location":"documentation/concept3/#3-neuartige-nicht-lineare-fusionsformel","title":"3. Neuartige nicht-lineare Fusionsformel","text":"<p>Um die Synergie zwischen mehreren hochgradigen Indikatoren (Line, Session, Graph) abzubilden und nicht nur linear zu summieren, definieren wir:</p> \\[ \\mathrm{score}_{\\mathrm{fusion}}(s_j)  = \\bigl[(\\mathrm{score}_{\\mathrm{line}}(s_j) + 1)^{\\alpha} \\cdot (\\mathrm{score}_{\\mathrm{session}}(s_j) + 1)^{\\beta} \\cdot (\\mathrm{score}_{\\mathrm{graph}}(s_j) + 1)^{\\gamma}\\bigr] - 1 \\] <p>Interpretation: - Alle Teil-Scores werden um 1 erh\u00f6ht, damit 0-Anomaly keine Nullmultiplikation verursacht. - Die Exponenten \\(\\alpha, \\beta, \\gamma\\) gewichten die Wichtigkeit der jeweiligen Ebene. - Ist \\( \\mathrm{score}_{\\mathrm{line}} \\approx 0\\), aber \\(\\mathrm{score}_{\\mathrm{session}}\\) und \\(\\mathrm{score}_{\\mathrm{graph}}\\) hoch, so bleibt das Produkt dennoch gro\u00df. - Synergetischer Effekt: Wenn zwei oder drei Teil-Scores gleichzeitig hoch sind, multiplizieren sich ihre Abweichungen, was den resultierenden Fusion-Score deutlich steigert (st\u00e4rker als in einer linearen Summe).</p> <p>Beispiel: - line=2, session=1, graph=0 =&gt; \\((2+1)^\\alpha (1+1)^\\beta (0+1)^\\gamma -1\\). - Sind alle &gt; 0 =&gt; starker Multiplikationseffekt.</p>"},{"location":"documentation/concept3/#4-adaptives-container-spawning","title":"4. Adaptives Container-Spawning","text":""},{"location":"documentation/concept3/#41-heuristisches-grundmodell","title":"4.1 Heuristisches Grundmodell","text":"<ul> <li>Falls \\(\\mathrm{score}_{\\mathrm{fusion}}(s_j) &gt; T\\) und Containerlimit nicht erreicht, wird ein passender Container hochgefahren (z.\u202fB. SSH, HTTP).  </li> <li>Ressourcenlimit: max \\(\\mathrm{Pods} = M\\).  </li> <li>Timeout: Nach \\(\\delta\\) min ohne neue Interaktion wird Container beendet.</li> </ul>"},{"location":"documentation/concept3/#42-reinforcement-learning-spater","title":"4.2 Reinforcement Learning (sp\u00e4ter)","text":"<ul> <li>RL-Agent liest \\(\\mathrm{score}_{\\mathrm{fusion}}\\) + Containerstatus.  </li> <li>Aktionen: spawn/stop service, adjust Banner, etc.  </li> <li>Reward( s_j ) = \\(\\mathrm{Datengewinn} - \\mathrm{Ressourcenaufwand}\\).  </li> <li>Expert-Override = negative/positive Korrektur in der Replay-Memory.</li> </ul>"},{"location":"documentation/concept3/#5-versionierung-von-angriffsverhalten","title":"5. Versionierung von Angriffsverhalten","text":""},{"location":"documentation/concept3/#51-bot-apt-familien","title":"5.1 Bot &amp; APT-Familien","text":"<ul> <li>Jede Familie hat eine Repr\u00e4sentation \\( f_{\\mathrm{fam}} \\in \\mathbb{R}^d \\).  </li> <li>Neue Session \\( e_{s_j} \\) =&gt; Distanz \\(\\mathrm{dist}(f_{\\mathrm{fam}}, e_{s_j})\\).  </li> <li>Gr\u00f6\u00dfer als \\(\\Delta\\) =&gt; neue Version. z.\u202fB. \u201cBot_X_v2\u201c.</li> </ul>"},{"location":"documentation/concept3/#52-bulk-step-summaries","title":"5.2 Bulk-Step Summaries","text":"<ul> <li>Gro\u00dfe repetitive Steps (z.\u202fB. 500 bruteforce-Versuche) werden in der Kette als ein \u201cBulk-Knoten\u201d aufgef\u00fchrt. </li> <li>Zeitliche Aggregation( \\(\\Delta t\\) ) =&gt; verschlankt die Visualisierung.</li> </ul>"},{"location":"documentation/concept3/#6-datenlage-bsi-logs-t-pot","title":"6. Datenlage: BSI-Logs &amp; T-Pot","text":""},{"location":"documentation/concept3/#61-bsi-honeypot-unlabeled-all-port","title":"6.1 BSI-Honeypot (unlabeled, All-Port)","text":"<ul> <li>Hauptquelle: Erfassung im realen Betrieb, massive Logs.  </li> <li>Kein oder kaum Ground-Truth =&gt; unsupervised Ans\u00e4tze.</li> </ul>"},{"location":"documentation/concept3/#62-t-pot-daten-ebenfalls-unlabeled-aber-session-structured","title":"6.2 T-Pot-Daten (ebenfalls unlabeled, aber session-structured)","text":"<ul> <li>Bieten Interaktionslogs (Cowrie, Dionaea etc.) =&gt; L\u00e4ngere Kommandosequenzen.  </li> <li>Optionales Vortraining (z.\u202fB. Transformer-LSTM auf \u201cSSH Kommandos\u201d).</li> <li>Auch hier kein direktes Label, aber man kann heuristische Pseudo-Labels (z.\u202fB. \u201csicherer vs. unsicherer Befehlssatz\u201d) definieren.</li> </ul>"},{"location":"documentation/concept3/#63-partial-testing","title":"6.3 Partial Testing:","text":"<ul> <li>Subsets von KDD Cup 99 / NSL-KDD, oder manuell gelabelte Ausschnitte =&gt; Basic Precision/Recall.  </li> </ul>"},{"location":"documentation/concept3/#7-evaluationskonzept","title":"7. Evaluationskonzept","text":"<ol> <li>Anomalie-Bewertung:</li> <li>Unsupervised Metriken (z.\u202fB. Reconstruction error distribution).  </li> <li> <p>Clustering-Qualit\u00e4t (Silhouette Score) bei Session-Embeddings.  </p> </li> <li> <p>Fusion-Formel: </p> </li> <li>Variation der Exponenten \\(\\alpha, \\beta, \\gamma\\).  </li> <li> <p>Test, ob synergy-Ansatz signifikant besser hochgradige Ketten markiert als lineare Summe.</p> </li> <li> <p>Ressourcen: </p> </li> <li> <p>\\(\\#\\text{Container}, \\mathrm{CPU/Mem}\\) pro Tag, RL-Policy vs. heuristische Policy.</p> </li> <li> <p>Versionierung:</p> </li> <li>Manuelle Stichproben =&gt; Ab wieviel \\(\\Delta\\) distanz entsteht eine neue Bot-Familie v2 vs. v3?  </li> <li>Metrik: Frequenz unn\u00f6tiger Version Splits vs. verpasster wirklicher Changes.</li> </ol>"},{"location":"documentation/concept3/#8-stufenweise-realisierung","title":"8. Stufenweise Realisierung","text":"<ol> <li>Line-level AE + simpler Container-Spawn (Score&gt;Schwelle =&gt; Start).  </li> <li>Session-Modul + synergy-Fusionsformel \\((\\dots)^\\alpha (\\dots)^\\beta (\\dots)^\\gamma -1\\).  </li> <li>Optionale GNN im HPC- oder Batchmodus f\u00fcr top-verd\u00e4chtige Sessions.  </li> <li>RL-Integration \u2192 Abl\u00f6sen heuristischer Container-Starts, Overriding durch Analysten.  </li> <li>Versionierung und Bulk Steps \u2192 Kettenvisualisierung iterativ verfeinert.</li> </ol>"},{"location":"documentation/concept3/#9-schlussbemerkung","title":"9. Schlussbemerkung","text":"<p>Dieses Konzept nutzt die BSI-Honeypot-Daten als prim\u00e4ren Treiber und T-Pot-Logs als zus\u00e4tzlichen Motor, um z.\u202fB. Kommandosequenzen f\u00fcr Session-Modelle zu trainieren oder Modelle zu validieren. Dabei beh\u00e4lt man die Komplexit\u00e4t im Griff, indem:</p> <ul> <li>(a) nur bei hohen Scores tiefergehende Analysen (Session, GNN) erfolgen,  </li> <li>(b) Container-Spawning streng limitiert ist,  </li> <li>(c) die neue synergy-Fusionsformel exponentiell ansteigt, sobald mehrere Ebenen gleichzeitig auff\u00e4llig sind,  </li> <li>(d) ein schrittweises Rollout (z.\u202fB. MVP \u2192 GNN \u2192 RL \u2192 Versionierung) erfolgt.</li> </ul> <p>Mit diesem Ansatz gelingt eine fortschrittliche, aber praktikable ML-basierte All-Port-Honeypot-L\u00f6sung, die durch die synergy-basierte Anomaliescore-Berechnung mehrdimensionale Abweichungen erkennt und zugleich ressourcenschonend in reale BSI-Prozesse integrierbar bleibt.</p>"},{"location":"documentation/journals/","title":"Journals","text":"Name Publisher SJR Quartil Fit Anspruch Cybersecurity Springer Open Q1/Q2 Sehr gut, Fokus auf Sicherheitsforschung, auch angewandt und politisch-strategisch (gut passend bei BSI-Beteiligung). Hoch ,aber springer open journals sind etwas zug\u00e4nglicher als ACM/IEEE. Gut m\u00f6glich bei solider Methodik und innovativem Thema. IEEE Access IEEE Q1 Generisch, aber sehr breit akzeptierend, auch Security/AI-Kombination m\u00f6glich. Hoch, aber auch Massenjournal mit relativ hohen Annahmeraten bei methodischer Sauberkeit und guter Struktur. Eher zahlen- und ergebnisgetrieben. Journal of Information Security and Applications Elsevier Q1 Sehr gut \u2013 genau der Bereich (praktische Anwendungen in der IT-Sicherheit). Mittel bis hoch. Elsevier hat nicht den besten Ruf, wird aber dennoch Peer-Reviewed. Annahmechancen gut bei praktischer Relevanz. Journal of Computer Virology and Hacking Techniques Springer Q3 Thematisch perfekt (Hacking-Techniken &amp; Malware, also Honeypots super relevant). Relativ einfache Annahme. Gut f\u00fcr erste Publikation, aber Ansehen eher gering. Digital Threats: Research and Practice ACM Q2 Extrem gut, da genau Sicherheitsbedrohungen und Praxis im Fokus. Mittel bis hoch. ACM Peer-Review sehr anspruchsvoll, aber DT:RP ist weniger elit\u00e4r als andere ACM-Journals. Applied Intelligence Springer Q2 Gut, Fokus auf angewandte KI. Honeypots + AI k\u00f6nnte gut reinpassen. (zu?) Hoch, wird Methodik, Experimente und Evaluation sehr kritisch pr\u00fcfen. IEEE Intelligent Systems IEEE Q1 M\u00f6glich, aber Fokus eher auf intelligente Systeme in einem breiten Kontext (weniger Security). Sehr/zu hoch, nur sehr innovative Arbeiten. Niedrige Annahmechancen ohne Top-Beitrag. SN Computer Science Springer Q2 Generisch genug, passt. Sicherheits- und KI-Themen sind h\u00e4ufig vertreten. Mittel, deutlich niedrigere H\u00fcrden als IEEE oder ACM, daf\u00fcr breites Themenspektrum. Gute Chance bei sauberer Arbeit. IEEE Transactions on Emerging Topics in Computational Intelligence (TETCI) IEEE Q1 Sehr gut, wenn der Schwerpunkt stark auf neuen KI-Methoden, innovativen Algorithmen oder adaptiven intelligenten Systemen liegt. Security als Anwendungsfeld wird akzeptiert, aber der Fokus muss klar auf \u201eemerging computational intelligence\u201c sein (z. B. neuartige ML-Architekturen oder RL-basierte Steuerung von Honeypots). Sehr/Zu hoch. Extrem anspruchsvoll, methodische Tiefe, mathematische Strenge und theoretische Innovation werden erwartet. Annahme nur wahrscheinlich bei hochinnovativen Ans\u00e4tzen und starker theoretischer Fundierung plus exzellenter empirischer Evaluation. IEEE Transactions on Information Forensics and Security (TIFS) IEEE Q1 Sehr passgenau f\u00fcr Security, Forensik und ML-Anwendungen. Fokus auf methodische Strenge und breite Anwendbarkeit (z. B. ML-basierte Intrusion Detection, Honeypots, Steganographie, Kryptographie etc.). Sehr/Zu hoch. F\u00fchrendes IEEE-Journal im Bereich Forensik/Security, erwartet fundierte theoretische Grundlage, umfassende Evaluation und hohe Innovationsh\u00f6he. Entsprechend kompetitives Peer-Review."},{"location":"documentation/math/","title":"Math","text":""},{"location":"documentation/math/#4-mathematical-background","title":"4. Mathematical Background","text":""},{"location":"documentation/math/#41-problem-setup","title":"4.1 Problem Setup","text":"<p>We consider honeypot log data consisting of a set of log lines: $$ L = {l_1, l_2, \\dots, l_n} $$ These lines are grouped into sessions: $$ S = {s_1, s_2, \\dots, s_m} $$ Each log line is embedded into a continuous vector space: $$ e_{l_i} \\in \\mathbb{R}^d $$ A session embedding is derived via a Transformer-LSTM encoder: $$ e_{s_i} = \\text{TransformerLSTM}(s_i) $$  </p>"},{"location":"documentation/math/#42-cross-session-anomaly-cohesion-score-csacs","title":"4.2 Cross-Session Anomaly Cohesion Score (CSACS)","text":"<p>We define the Cross-Session Anomaly Cohesion Score (CSACS) for a community \\( C \\subseteq S \\) as: $$ CSACS(C) = \\frac{ \\sum_{(s_i, s_j) \\in E_C} \\left( \\lambda_1 \\cdot \\frac{1}{D(s_i, s_j) + \\epsilon} + \\lambda_2 \\cdot \\frac{1}{T(s_i, s_j) + \\delta} \\right) \\cdot \\min(A(s_i), A(s_j)) }{ |E_C| } $$ Where: - \\(E_C\\) is the edge set induced by \\(C.\\) - \\(D(s_i, s_j)\\) is the Euclidean distance between session embeddings. - \\(T(s_i, s_j)\\) is the temporal gap between sessions. - \\(A(s_i)\\) is the anomaly score of session \\(s_i.\\) - \\(\\lambda_1, \\lambda_2, \\epsilon, \\delta &gt; 0\\) are weighting and stability parameters.  </p>"},{"location":"documentation/math/#43-optimization-objective","title":"4.3 Optimization Objective","text":"<p>The objective in identifying clusters is to maximize: $$ \\max_{C} CSACS(C) - \\gamma \\cdot |C| $$ with \\(\\gamma &gt; 0\\) serving as a size penalty to avoid trivial large clusters.  </p>"},{"location":"documentation/math/#44-statistical-stability-under-a-null-model","title":"4.4 Statistical Stability Under a Null Model","text":""},{"location":"documentation/math/#441-null-model-setup","title":"4.4.1 Null Model Setup","text":"<p>We analyze the probability that a randomly formed cluster achieves high cohesion under a null hypothesis in which no structured anomaly exists. We assume:  </p> <ul> <li> <p>Embedding noise model: $$ e_{s_i} = \\mu_{s_i} + \\eta_{s_i}, \\quad \\eta_{s_i} \\sim \\mathcal{N}(0, \\sigma^2 I_d) $$ with independent noise \\(\\eta_{s_i}.\\) </p> </li> <li> <p>Temporal gaps: Independent random variables with density \\(f_T(t)\\) bounded by \\(\\frac{1}{T_{\\max}}\\) on \\([0, T_{\\max}]\\).  </p> </li> <li> <p>Anomaly scores: Independent, bounded random variables in \\([0, A_{\\max}]\\), independent of embeddings and temporal gaps.  </p> </li> <li> <p>Clusters: Community \\(C\\) and edge set \\(E_C\\) are selected independently of all random variables described.  </p> </li> </ul> <p>Interpretation: This null model forms the baseline scenario against which discovered clusters can be tested. High cohesion under this model is highly unlikely; observed deviations indicate structure.  </p>"},{"location":"documentation/math/#442-lemma-dominance-of-exponential-tails","title":"4.4.2 Lemma: Dominance of Exponential Tails","text":"<p>Lemma 1 (Tail Dominance): Let \\(X = \\left( \\lambda_1 \\cdot \\frac{1}{D + \\epsilon} + \\lambda_2 \\cdot \\frac{1}{T + \\delta} \\right) \\cdot A,\\) where: - \\(D^2 \\sim 2 \\sigma^2 \\chi^2_d,\\) - \\(T \\sim f_T\\) bounded by \\(\\frac{1}{T_{\\max}}\\), - \\(A \\in [0, A_{\\max}]\\) is independent.  </p> <p>Then \\(X\\) is sub-exponential with exponential tail decay dominated by the distance inversion term. In particular, there exist constants \\(c_1, c_2 &gt; 0\\) such that for large \\(x\\): $$ P(X &gt; x) \\leq c_1 \\exp(-c_2 x^2) $$  </p> <p>Proof Sketch: - Distance tail deviations decay exponentially in \\(d.\\) - Temporal term is polynomially decaying but multiplied by a random bounded variable and added to an exponentially decaying component. - By known results in convolution of distributions (Boucheron\u2013Lugosi\u2013Massart, Lemma 2.7), exponential tails dominate.  </p>"},{"location":"documentation/math/#443-theorem-statistical-stability-of-csacs","title":"4.4.3 Theorem: Statistical Stability of CSACS","text":"<p>Theorem (Statistical Stability Under the Null): Under the null model described, let \\(C\\) be a random community with \\(|E_C|\\) edges. Suppose \\(\\epsilon &gt; c_1 \\sigma,\\ \\delta &gt; c_2 / T_{\\max}.\\) Then for any threshold \\(\\theta &gt; 0,\\) $$ P\\bigl(CSACS(C) \\geq \\theta \\bigr) \\leq \\exp\\bigl(- \\alpha \\cdot |E_C| \\cdot \\theta^2 \\bigr), $$ where \\(\\alpha &gt; 0\\) depends on \\(\\sigma^2, d, \\lambda_1, \\lambda_2, A_{\\max}, c_1, c_2, T_{\\max}.\\) </p> <p>Corollary: As dimension \\(d\\) increases, \\(\\alpha\\) scales proportionally with \\(d,\\) reinforcing stability in high-dimensional embeddings.  </p>"},{"location":"documentation/math/#444-proof-outline","title":"4.4.4 Proof (Outline)","text":"<ol> <li>Individual edge contributions follow sub-exponential tails due to Lemma 1.  </li> <li>Sums of independent sub-exponential random variables concentrate according to Vershynin (2018), Theorem 2.7.1: $$ P\\left( \\sum_{(i,j) \\in E_C} X_{(i,j)} \\geq |E_C| \\theta \\right) \\leq \\exp\\left(-c |E_C| \\cdot \\min\\left(\\frac{\\theta^2}{v^2}, \\frac{\\theta}{M}\\right) \\right) $$ where \\(v^2\\) is total variance and \\(M\\) bounds the tails.  </li> <li>All constants can be expressed in terms of \\(\\sigma, d, A_{\\max}, T_{\\max}, \\lambda_1, \\lambda_2.\\) </li> </ol>"},{"location":"documentation/math/#445-explicit-constant","title":"4.4.5 Explicit Constant","text":"<p>An explicit lower bound on \\(\\alpha\\) is given by: $$ \\alpha = c \\cdot \\min \\left( \\frac{d \\lambda_1^2}{\\sigma^2}, \\frac{\\lambda_2^2 \\delta^2}{T_{\\max}^2}, \\frac{1}{A_{\\max}^2} \\right) $$ with universal constant \\(c &gt; 0.\\) </p>"},{"location":"documentation/math/#446-hypothesis-testing-interpretation","title":"4.4.6 Hypothesis Testing Interpretation","text":"<p>The stability theorem provides a null hypothesis threshold: if an observed cluster exceeds $$ CSACS(C) &gt; \\theta_{\\text{crit}} $$ with \\(\\theta_{\\text{crit}}\\) chosen such that $$ \\exp\\left(- \\alpha \\cdot |E_C| \\cdot \\theta_{\\text{crit}}^2 \\right) &lt; \\delta $$ (for some desired significance level \\(\\delta\\)), we can reject the null hypothesis that the cluster is random.  </p> <p>This reframes CSACS detection as a hypothesis testing procedure rather than relying on strong assumptions about the clustering algorithm.  </p>"},{"location":"documentation/math/#447-search-space-correction","title":"4.4.7 Search Space Correction","text":"<p>If the clustering algorithm searches over a candidate set \\(\\mathcal{C}\\) of communities with \\(|\\mathcal{C}|\\) polynomial in \\(m,\\) a union bound corrects for search: $$ P\\left( \\exists C \\in \\mathcal{C}: CSACS(C) \\geq \\theta \\right) \\leq |\\mathcal{C}| \\cdot \\exp\\bigl(- \\alpha \\cdot |E_C| \\cdot \\theta^2 \\bigr) $$ For polynomial-sized \\(|\\mathcal{C}|\\), the exponential decay dominates for large thresholds.  </p>"},{"location":"documentation/math/#448-limitations-and-future-work","title":"4.4.8 Limitations and Future Work","text":"<ul> <li>The result applies under independence assumptions. In practice, embeddings are correlated outputs of deep models.  </li> <li>Extending stability results to dependent embeddings and data-conditioned cluster search remains open. This will likely require bounding the complexity of clustering algorithms (e.g., via VC dimension or covering number arguments).  </li> <li>Heavier-tailed temporal distributions and bursty behavior are not covered here; extending to sub-exponential temporal gap distributions is part of ongoing theoretical work.  </li> </ul>"},{"location":"documentation/math/#references","title":"References","text":"<ul> <li>Vershynin, R. (2018). High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press.  </li> <li>Boucheron, S., Lugosi, G., &amp; Massart, P. (2013). Concentration Inequalities: A Nonasymptotic Theory of Independence. Oxford University Press.  </li> </ul>"},{"location":"documentation/relatedWork/","title":"Related Work","text":"Name Content Relevance Journal/Conference Publisher Personal Rating Link to annotated paper Year Cites An Unsupervised Deep Learning Model for Early Network Traffic Anomaly Detection D-PACK: A deep-learning-based anomaly detection system for IoT traffic. Combines a Convolutional Neural Network (CNN) with an unsupervised Autoencoder to automatically learn traffic patterns and detect malicious flows. Instead of relying on manual, flow-level features or signatures, D-PACK analyzes only the first few bytes of the first two packets of each flow, enabling ultra-early detection. Demonstrates near 100% detection accuracy with a very low false-positive rate (0.83%) in large-scale DDoS scenarios like Mirai-based attacks. Designed for real-time, resource-efficient mitigation of abnormal traffic. This approach could be helpful, as it utilizes the first bytes of the network packet payload. It might be worth exploring whether I can integrate this method and combine it with other techniques to enhance detection accuracy and keep the model slim. IEEE Access IEEE \u2b50\u2b50\u2b50 Click Me 2020 159 Anomaly Detection From Log Files Using Unsupervised Deep Learning Unsupervised LSTM Autoencoder model that processes raw log text without preprocessing or handcrafted features. It outputs an anomaly score per log entry, reflecting content and temporal rarity. Trained on 1M HDFS log lines and tested on 1M lines. Acts as a coarse filter for anomaly detection when labeled data is unavailable. The approach has not been applied to security-related data. However, it aligns well with my concept, as it combines temporal context modeling through LSTMs with an autoencoder. Additionally, it may offer valuable insights, given that it operates on raw, unlabeled data without requiring preprocessing \u2014 matching the conditions of my use case. Formal Methods. FM 2019 International Workshops Springer \u2b50\u2b50\u2b50\u2b50 Click Me 2020 DeepLog: Anomaly Detection and Diagnosis from System Logs through Deep Learning DeepLog: Uses an LSTM-based deep learning model to treat system logs as language sequences. Automatically learns normal log patterns and detects anomalies when deviations occur. Supports incremental online updates to adapt to new patterns over time. Builds workflows from logs to aid in root cause analysis. Outperforms traditional log-based anomaly detection methods in large-scale experiments. While DeepLog presents an elegant approach by modeling system logs as sequences using LSTM-based architectures and autoencoders to learn normal patterns, it also has significant limitations in the context of this work. The entire approach is focused on structured system logs and does not extend to network traffic, interactive attacker behavior, or unstructured data streams. This log-centric design makes it far from suitable for honeypot or intrusion detection scenarios, where anomalies frequently occur in network flows, command sequences, or multi-modal attacker interactions. Additionally, the model is not designed to handle complex, heterogeneous data sources typical in security monitoring setups, and it lacks flexibility for incorporating contextual or behavioral information beyond static logs. Therefore, despite being a prominent example of sequence-based anomaly detection, its applicability in honeypot-based or network-level anomaly detection is limited. CCS '17: Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security ACM \u2b50\u2b50 Click Me 2017 1055 Unsupervised Machine Learning Techniques for Network Intrusion Detection on Modern Data The paper compares four unsupervised models (PCA, Isolation Forest, One-Class SVM, and Autoencoder) for intrusion detection on the CIC-IDS-2017 dataset. All models are trained on benign traffic only, focusing on zero-day attack detection. The autoencoder outperforms others with an AUROC of 0.9775 and an F1 score of 0.9616, combining high recall and precision. The OC-SVM is best when low false positives are critical. The study highlights the trade-off between detection performance and computational efficiency, with all models being fast and suitable for real-time deployment. Autoencoders and OC-SVMs are recommended for robust, adaptive NIDS. This paper is highly relevant to my thesis as it provides a structured comparison between several unsupervised learning algorithms for intrusion detection, directly addressing the challenge of detecting unknown, zero-day attacks. The evaluation on CIC-IDS-2017 aligns with my focus on modern, realistic datasets. In particular, the paper's demonstration of autoencoders' ability to capture complex attack patterns without manual feature engineering supports my approach of using deep learning for feature extraction in honeypot-based environments. Moreover, the discussion on computational overhead, model optimization, and threshold selection provides valuable insights for practical system design. The results showing the superior balance between detection accuracy and runtime efficiency of autoencoders can guide my architecture choice. The OC-SVM\u2019s performance in minimizing false positives also offers potential for integration as a fallback or secondary detection stage in my work. 2020 4th Cyber Security in Networking Conference (CSNet) IEEE \u2b50\u2b50\u2b50\u2b50 Click Me 2020 26 A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks The authors test their RNN-based IDS (RNN-IDS) on the NSL-KDD dataset for both binary (normal vs. anomaly) and multiclass classification (normal, DoS, R2L, U2R, Probe). They preprocess the dataset with feature encoding and normalization and use a fully connected RNN model. The paper compares RNN-IDS against traditional machine learning algorithms (J48, SVM, Random Forest, ANN) and a reduced-size RNN approach from prior work. Results show that RNN-IDS outperforms classical methods in accuracy, detection rate, and false-positive rate, although training times are longer (not GPU-enabled). The authors suggest that with GPU acceleration and advanced architectures (LSTM, BiRNNs), even better results are achievable. This paper is helpful for my thesis in multiple ways. First, it demonstrates that even relatively simple RNN architectures can outperform classical machine learning models on benchmark datasets like NSL-KDD \u2014 without GPU acceleration and using fairly basic setups. This reassures me that more modern approaches (GPU training, LSTMs, attention mechanisms) will easily surpass those results. Second, the paper is very technical and methodical; it explains each step, including dataset structure, feature preprocessing, and hyperparameter tuning. This shows me how straightforward it is to fill out these sections in my own thesis. Finally, by showing performance analysis in both binary and multiclass classification with confusion matrices, training times, and metrics like accuracy, TPR, and FPR, it sets a clear template for how I can structure and present my own results in a way that will be accepted as thorough and rigorous. IEEE Access IEEE \u2b50\u2b50 Click Me 2017 1228 AutoLog: Anomaly detection by deep autoencoding of system logs AutoLog proposes a semi-supervised deep autoencoder model that uses entropy-based scoring on log chunks from heterogeneous systems. Scores from normal operations train the model; deviations are detected via reconstruction error. It does not rely on log structure or templates and works across distributed systems. Evaluated on industrial, microservices, BG/L supercomputer, and Hadoop logs, AutoLog achieved recall between 0.96 and 0.99 and precision between 0.93 and 0.98, outperforming isolation forest, one-class SVM, decision trees, and variational autoencoders. This work is relevant due to its template-independent design and ability to handle heterogeneous, unstructured log data without prior feature engineering. The entropy-based scoring combined with deep autoencoding offers a robust method for anomaly detection in noisy, multi-source environments. This aligns with my approach for adaptive anomaly detection in dynamic, complex honeypot setups, where structured features are not always available. Expert Systems with Applications Elsevier \u2b50\u2b50\u2b50 Click Me 2021 An anomaly detection method to detect web attacks using Stacked Auto-Encoder Proposes an anomaly detection method for web attacks using a stacked autoencoder (SAE) for feature extraction and isolation forest as a one-class classifier. Uses character-level n-gram models for feature construction (mostly unigram and bigram), which suffers from high dimensionality. SAE architecture consists of layers with 1000, 400, and 100 hidden neurons, quadratic loss function, and experiments with different optimizers (Adam, RMSProp, etc.). Results show improvement over simple n-gram models, but the paper is written in poor English, uses outdated hardware, and lacks depth. Only marginally relevant. The approach (SAE + isolation forest) is interesting but not innovative. Mostly helpful for fine-tuning my research gap and showing how not to design evaluation setups. Demonstrates the difference between short conference papers and more thorough research. 2018 6th Iranian Joint Congress on Fuzzy and Intelligent Systems (CFIS) IEEE \u2b50 Click Me 2018 73 Anomaly Detection for HTTP Using Convolutional Autoencoders Proposes a novel anomaly detection approach for HTTP traffic using a convolutional autoencoder (CAE) combined with character-level binary image transformation. Instead of manual feature engineering, HTTP messages are converted into binary \"images\" representing characters, which are then processed by a modified Inception-ResNet-v2 CAE. Detection is based on reconstruction errors (BCE) and a novel decision metric, binary cross varentropy (BCV), which captures variance properties. The model is trained unsupervised on normal HTTP traffic and achieves superior performance compared to one-class SVMs, Isolation Forest, and a shallower CREPE-based CAE. The paper also evaluates character embedding but finds minimal performance gains. Trained on ~129,000 HTTP messages and evaluated on ~26,000 messages. Demonstrates strong results with low false-positive rates (~4% at TPR 0.99) and high MCC/F1. Very interesting due to the use of CAEs with character-level transformation, showing that feature engineering can be avoided. The use of BCV as a decision metric is novel and may offer inspiration for alternative scoring metrics in my honeypot setup. However, the approach is tailored to structured HTTP data and image-like transformations; it\u2019s unlikely that this technique would generalize to packet-level byte streams or more complex, mixed traffic. Additionally, their infrastructure relies on large GPU resources (4\u00d7 Tesla P100), which might not scale well for real-time inference without significant optimization. Overall, the paper helps to sharpen my research gap: my focus is on packet-level raw byte processing, multi-modal attacker behavior, and resource-efficient, real-time detection rather than protocol-specific image encodings IEEE Access IEEE \u2b50\u2b50\u2b50\u2b50 Click Me 2018 33 A Deep Learning Approach to Network Intrusion Detection This paper presents a novel intrusion detection approach using multi-layer non-symmetric deep auto-encoders (NDAEs) for unsupervised feature learning, followed by a shallow learning classifier based on Random Forest. The architecture consists of multiple stacked NDAEs for deep feature extraction, after which the learned representations are classified using a Random Forest model. The system is implemented in GPU-enabled TensorFlow and evaluated on the KDD Cup '99 and NSL-KDD datasets. Results demonstrate significant improvements in detection accuracy and false positive rates compared to traditional machine learning approaches, showcasing its potential for application in modern network intrusion detection systems. This paper is highly relevant, as it presents a deep feature extraction approach combined with a lightweight classifier (Random Forest). The concept of using unsupervised representation learning and passing the results to a separate shallow classifier aligns well with my idea of balancing deep learning and efficiency. The methodology could serve as a foundation for combining unsupervised feature learning with scalable classifiers in modern honeypot-based anomaly detection. The use of the kdd dataset and the reference to other paperps who used this dataset could be helpful for my evaluation. The overall structure could be helpful for my paper as well. IEEE Transactions on Emerging Topics in Computational Intelligence IEEE \u2b50\u2b50\u2b50\u2b50 Click Me 2017 1039 An LSTM-Based Deep Learning Approach for Classifying Malicious Traffic at the Packet Level The paper presents a packet-level intrusion detection model using word embeddings and a three-layer LSTM architecture. Each packet is parsed into a fixed 54-byte representation, converted into \"sentences\" of header fields, and embedded for input into the LSTM. The model is trained on large datasets (ISCX2012, USTC-TFC2016, Mirai-RGU, and self-collected Mirai-CCU), demonstrating extremely high accuracy (near 100%) on both training and validation sets. The focus on real-time detection without flow aggregation is innovative. However, the model's practicality is questionable due to heavy GPU requirements (Tesla K80) and large model complexity (&gt;4 million parameters). Evaluation lacks discussion on handling encrypted payloads or highly variable traffic, and no adversarial robustness evaluation is provided. Relevant as it shows packet-level detection without requiring flow reconstruction, using LSTMs on raw headers. However, the paper mainly focuses on syntactic structure and known datasets. Their embedding strategy is interesting but heavily handcrafted and limited to static header structures. It does not generalize well to dynamic payload-based anomalies or encrypted/obfuscated traffic. This reinforces my research gap: going beyond static header field embeddings towards dynamic, payload-level learning and multi-modal behavior modeling. Applied sciences Basel : MDPI AG \u2b50\u2b50\u2b50 Click Me 2020 A Machine Learning Approach to Classify Network Traffic The paper focuses on classifying benign vs. darknet traffic using the CIC-Darknet 2020 dataset. The authors apply preprocessing (PCA for dimensionality reduction), balance the dataset using SMOTE, and compare various classical machine learning algorithms (e.g., Random Forest, Decision Tree, Extra Trees, AdaBoost). The best results are achieved by Decision Tree and Extra Trees classifiers, with near 100% accuracy and MCC. The paper lacks any deep learning approaches, does not handle real-time performance evaluation, and relies on feature-based manual preprocessing rather than automated feature learning. Only slightly relevant. It provides a good overview of classical machine learning baselines for traffic classification, which can serve as a comparison point for deep learning-based approaches. However, the approach is heavily dependent on manual feature extraction (via PCA and dataset features), lacks discussion on real-time applicability, adversarial robustness, or packet-level raw data processing. The work confirms that classical methods are useful but also highlights their limitations for dynamic or unknown attack patterns. 13th International Conference on ELECTRONICS, COMPUTERS and ARTIFICIAL INTELLIGENCE \u2013 ECAI-2021 International Conference on ELECTRONICS, COMPUTERS and ARTIFICIAL INTELLIGENCE \u2013 ECAI \u2b50 Click Me 2021 7 Raw Packet Data Ingestion with Transformers for Malicious Activity Classifications This paper proposes using the ByT5 transformer \u2014 a token-free, byte-level NLP model \u2014 for direct classification of raw packet data as malicious or benign without manual feature engineering or tokenization. The model is fine-tuned on the ISOT dataset and evaluated across multiple days with different malicious traffic compositions. Achieves a maximum recall of 0.834 and F1 score of 0.693. Shows that short truncated packets (100 bytes) yield better results than larger truncations. The paper emphasizes the advantage of direct byte ingestion but also notes large resource requirements (300M parameter model, Tesla V100 GPUs), long training times (37 hours per epoch), and convergence issues (vanishing gradients). Highly relevant, as this paper is among the first to apply large transformer models directly on raw packet data \u2014 very close to my goal. It demonstrates that direct byte-level learning is feasible but computationally heavy. The results are not yet production-ready (F1 = 0.693), and real-time deployment issues are not addressed. This reinforces my gap: developing more lightweight architectures or hybrid models that can achieve comparable detection results without requiring massive infrastructure. The discussion around training complexities, truncation, and input sequence lengths is very helpful. 2023 International Conference on Machine Learning and Applications (ICMLA) International Conference on Machine Learning and Applications (ICMLA) \u2b50\u2b50\u2b50\u2b50 Click Me 2023 0 Hybrid System Between Anomaly Based Detection System and Honeypot to Detect Zero Day Attack The paper discusses the limitations and strengths of anomaly-based detection systems and honeypots and proposes a hybrid model to detect zero-day attacks more effectively. The proposed system uses honeypots to lure attackers and gather behavior data, while anomaly-based systems detect deviations from learned normal behavior. They suggest feeding honeypot observations back into the anomaly detection system to improve accuracy and reduce false positives. However, the paper remains conceptual without presenting concrete experimental validation or implementation details. Conceptually interesting, as it aligns with the idea of dynamic, feedback-based anomaly detection. But the paper is weak in terms of experimental contribution and lacks any implementation or performance evaluation. It can serve as theoretical support for integrating honeypot data into anomaly detection models, but adds little technical depth. It reinforces my focus on actually implementing and validating such hybrid systems with deep learning components. 2018 21st Saudi Computer Society National Computer Conference (NCC) IEEE \u2b50 Click Me 2018 11 A Near Real-Time Algorithm for Autonomous Identification and Characterization of Honeypot Attacks The paper presents UNADA, an unsupervised anomaly detection and characterization algorithm designed for honeypot traffic. It uses sub-space clustering, evidence accumulation, and inter-cluster correlation to identify and characterize attacks from unlabeled honeypot traffic in near real-time. Signatures are automatically generated and can be used to configure firewalls or routers autonomously. The algorithm is evaluated on real-world data from the University of Maryland, showing high detection accuracy and efficient parallelization for scalability. The work addresses both classification and risk-based prioritization of detected anomalies. Highly relevant. It demonstrates an unsupervised clustering-based approach for automatically identifying and characterizing honeypot-based attacks. The combination of clustering ensemble methods and signature generation is interesting and confirms the value of using honeypot data in combination with unsupervised ML techniques. However, the paper focuses on flow-level NetFlow data and classical clustering techniques rather than deep learning. This highlights a gap that my research addresses: leveraging packet-level raw data and neural architectures for autonomous anomaly detection, rather than relying on flow aggregation and handcrafted features. ASIA CCS '15: Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security ACM \u2b50\u2b50\u2b50\u2b50\u2b50 Click Me 2015 A Comparative Study of Unsupervised Anomaly Detection Techniques Using Honeypot Data The paper presents an extensive comparative analysis of eight unsupervised anomaly detection techniques (three outlier detection methods, four clustering approaches, and one-class SVM) using real-world honeypot traffic data from Kyoto University. The study evaluates detection accuracy, false positive rates, robustness to noise, training data size impact, detection of unknown attacks, and time complexity. Key findings: clustering-based methods outperform outlier detection for IDS purposes; DBScan and LOF are computationally heavy; Chebyshev and Euclidean distances are most suitable similarity metrics; and one-class SVM performs in between clustering and outlier detection methods. While the methodology is solid and the use of real honeypot data was ahead of its time, the paper is now over 15 years old and focused entirely on classical machine learning methods without any consideration of deep learning, modern high-throughput environments, or encrypted traffic. This strongly emphasizes the need for updated approaches that leverage deep neural models and raw packet data to handle current attack vectors Still relevant for understanding the evolution of anomaly detection and as a reference point for older methods. However, its age and focus on classical methods make it insufficient for addressing modern, dynamic attack landscapes. It reinforces my research gap: integrating deep learning-based packet-level detection in honeypot scenarios, moving beyond handcrafted features and clustering methods. IEICE Transactions on Information and Systems VOL.E93\u2013D, NO.9 SEPTEMBER 2010 IEICE \u2b50\u2b50\u2b50\u2b50\u2b50 Click Me 2010 2 FedNIDS: A Federated Learning Framework for Packet-Based Network Intrusion Detection System TFedNIDS proposes a two-stage federated learning framework combining decentralized training on packet-based data (using a DNN) and subsequent fine-tuning for novel attack detection. It addresses challenges with non-IID data distributions and adapts rapidly (in ~4 rounds) to zero-day attacks. The paper demonstrates high accuracy (F1 = 0.97) on CIC-IDS2017/2018 datasets and robust defense against adversarial attacks after fine-tuning. It uses raw packet features (normalized byte values) instead of handcrafted flow features. While impressive, it still requires substantial infrastructure and focuses on supervised DNN classification with federated aggregation, rather than unsupervised or self-supervised methods for anomaly detection. Highly relevant. It shows current progress in federated NIDS on packet-level data, emphasizing scalability and adaptability. However, the focus is on federated supervised classification, not unsupervised detection or lightweight models. My research gap lies in creating real-time-capable, unsupervised, or self-supervised transformer/autoencoder hybrids for honeypot environments, where labeled attack data is not guaranteed, and lightweight inference is critical. FedNIDS also does not address encrypted payload handling or multi-modal attacker behavior detection. Digital Threats: Research and Practice, Vol. 6, No. 1, Article 4 (February 2025) ACM \u2b50\u2b50\u2b50\u2b50 Click Me 2025 Improving Adaptive Honeypot Functionality with Efficient Reinforcement Learning Parameters for Automated Malware Proposes parameter tuning for RL agents in adaptive honeypots. Explores discount factor and learning rate settings in Q-learning setups to improve response behavior against malware. Evaluates against simulated attacks. Minor technical relevance but helpful for parameter optimization. Offers hints on tuning Q-table based agents for better learning speed and stability. Journal of Cyber Security Technology Taylor &amp; Francis \u2b50\u2b50 Click Me 2018 \u2014 Using Reinforcement Learning to Conceal Honeypot Functionality Uses Q-learning to tune honeypot responses and delay detection by attackers. Focuses on balancing stealth (concealment) with engagement. Evaluates the timing and response manipulation to reduce detectability. Relevant for RL-based deception tactics. Complements my goal of increasing attacker trust in the honeypot through learned response realism. AIxIA 2018, Italian Conference on Artificial Intelligence Springer \u2b50\u2b50\u2b50 Click Me 2018 \u2014 New Framework for Adaptive and Agile Honeypots Proposes HARM: honeypots using reinforcement learning to handle repetitive malware like worms. Introduces agile policy updates, automated redeployment, and captures attacker interaction via Q-learning/SARSA. Evaluates state-action space for malware behavior. Supports dynamic policy learning and system redeployment, relevant for container-based honeypots. Shows benefits of agility and adaptability in active threat environments. ETRI Journal Wiley \u2b50\u2b50\u2b50\u2b50 Click Me 2020 \u2014 A Comparison of an Adaptive Self-Guarded Honeypot with Conventional Honeypots Compares Asgard and Midgard (adaptive SSH honeypots with Q-learning) to Cowrie and real Linux. Focuses on trade-off between attacker engagement and containment. Asgard uses state-action dependent rewards for fine-grained policy learning. Directly aligns with my goals: adaptive interaction, attacker deception, and safe containment. Highlights design strategies for maximizing data collection while preventing full system compromise. Applied Sciences MDPI \u2b50\u2b50\u2b50\u2b50 Click Me 2022 \u2014 Evaluation of Reinforcement Learning Algorithm on SSH Honeypot The paper evaluates the impact of reinforcement learning on SSH honeypots using Cowrie. It aims to increase the duration and depth of attacker interaction by learning behavioral sequences before certain commands (e.g. download). It explores how RL can guide interaction policies and discusses the reward function design for deeper attacker engagement. Helpful as a basic empirical test of RL in honeypots, especially focused on SSH. Offers insight into sequence-based reward strategies and attacker behavior profiling. Can inform RL reward tuning in my framework. 2022 6th International Conference on Information Technology, Information Systems and Electrical Engineering (ICITISEE) IEEE \u2b50\u2b50\u2b50 Click Me 2022 \u2014 Asguard: Adaptive Self-guarded Honeypot opinioin journal rating annotated link year Deep Reinforcement Learning for Building Honeypots Against Runtime DoS Attack Introduces DARLH, a system combining deep reinforcement learning with IDS agents to respond to DoS attacks. Uses Deep RNNs and event tracking on datasets like UNSW-NB20 and Bot-IoT. Evaluates DARLH against prior methods like Na\u00efve Bayes Honeypot and Blockchain-based systems, showing 5\u201310% performance gains. Valuable for the use of DRL in real-time detection with structured datasets. Though more focused on DoS, it informs model architectures and agent behaviors in honeypot-based defense. Could inspire hybrid detection components in adaptive honeypots. Int. J. of Intelligent Systems Wiley \u2b50\u2b50\u2b50\u2b50 Click Me 2021 \u2014 RASSH \u2013 Reinforced Adaptive SSH Honeypot Proposes RASSH, a medium-interaction SSH honeypot using SARSA and Markov models to adapt based on attacker commands. Implemented in Python using PyBrain and Kippo as base. The system learns to keep attackers engaged while detecting typical behaviors. Foundational for RL-based honeypots. Demonstrates early, working application of SARSA to attacker behavior. Can guide state-action modeling and inspire comparisons with modern RL techniques. 2014 International Conference on Communications (COMM) IEEE \u2b50\u2b50\u2b50\u2b50 Click Me 2014 \u2014 Adaptive and Self-Configurable Honeypots One of the first high-interaction honeypots using reinforcement learning. Based on UML and a probabilistic automaton of attacker behavior. Adapts program responses (allow/block/modify) based on attack state. Focuses on self-management and attacker deception. Seminal work in adaptive honeypots. Introduces attacker-driven feedback loop via RL and explores adversarial learning. Supports the case for dynamic honeypot control policies. 12th IFIP/IEEE International Symposium on Integrated Network Management IEEE \u2b50\u2b50\u2b50\u2b50\u2b50 Click Me 2011 \u2014 HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning HoneyIoT uses MDP modeling and reinforcement learning to mimic IoT devices and evade detection. Learns optimal attacker engagement via attack trace replay and differential response mutation. Covert against honeypot detection tools, deployed publicly. Very relevant due to adaptive interaction design. Shows strong RL use in realistic honeypot deployment. Supports my goal of making attack sessions longer and more informative. WiSec '23: ACM Conference on Security and Privacy in Wireless and Mobile Networks ACM \u2b50\u2b50\u2b50\u2b50\u2b50 Click Me 2023 \u2014 Adaptive Honeypot Engagement through Reinforcement Learning of Semi-Markov Decision Processes opinioin journal rating annotated link year Reinforcement Learning-assisted Threshold Optimization for Dynamic Honeypot Adaptation to Enhance IoBT Networks Security opinioin journal rating annotated link year QRASSH - A Self-Adaptive SSH Honeypot Driven by Q-Learning opinioin journal rating annotated link year On the Rewards of Self-Adaptive IoT Honeypots Describes IRASSH-T, a self-adaptive honeypot using Inverse Reinforcement Learning (IRL) to model attacker behavior and derive optimal reward functions. Applied to SSH and Telnet-based IoT honeypots. Focuses on learning behavior patterns like Mirai botnet actions. Important for reward design and IRL perspective. Shows how real attacker behavior can be modeled and used to train RL agents effectively, even without explicit labels. Annals of Telecommunications Springer \u2b50\u2b50\u2b50\u2b50 Click Me 2019 \u2014 Generative AI SSH Honeypot With Reinforcement Learning opinioin 14th IEEE International Conference on Communication Systems and Network Technologies 2025 rating annotated link 2025 Playing Atari with Deep Reinforcement Learning DeepMind first DQN journal rating annotated link 2013 Q-learning first Q-Learning Springer Nature Machine learning rating annotated link 1992"},{"location":"documentation/rl/","title":"Adaptive Container-Orchestrierung mit Reinforcement Learning (RL)","text":""},{"location":"documentation/rl/#motivation","title":"Motivation","text":"<p>Statische Honeypot-Ans\u00e4tze wie T-Pot bieten dauerhaft eine Vielzahl von Services auf festen Ports an \u2013 unabh\u00e4ngig davon, ob ein Angreifer tats\u00e4chlich mit ihnen interagieren m\u00f6chte. Diese Strategie erzeugt zwar gro\u00dfe Mengen an Daten, f\u00fchrt jedoch zu:</p> <ul> <li>hoher Ressourcenlast durch permanent laufende Container</li> <li>reduzierter Tarnung (alle Ports offen = Honeypot-Indikator)</li> <li>vielen irrelevanten Sessions (z.\u202fB. durch einfache Scanner oder Bots)</li> </ul> <p>Um diese Nachteile zu \u00fcberwinden, setzt unser System auf adaptive Containerbereitstellung mithilfe von Reinforcement Learning (RL). Ziel ist es, nur dann einen Emulationscontainer zu starten, wenn First-Flight-Daten einer Quell-IP ein hohes Potenzial f\u00fcr wertvolle Angreiferinteraktionen aufweisen.</p>"},{"location":"documentation/rl/#vorteile-des-rl-basierten-deployments-gegenuber-statischen-systemen","title":"\u2705 Vorteile des RL-basierten Deployments gegen\u00fcber statischen Systemen","text":"Kriterium Statisches System (z.\u202fB. T-Pot) RL-basiertes System Ressourceneffizienz Gering Hoch Tarnung Gering (alle Ports offen) Hoch (on-demand-Emulation) Datenqualit\u00e4t Viel Bot-Rauschen Fokus auf wertvolle Interaktionen Reaktion auf Angreiferverhalten Keine Host-spezifisch, lernf\u00e4hig Skalierbarkeit Begrenzt durch Last Dynamisch skalierbar"},{"location":"documentation/rl/#entscheidungsgrundlage-first-flight-daten","title":"Entscheidungsgrundlage: First-Flight-Daten","text":"<p>Der RL-Agent trifft seine Entscheidungen auf Basis sogenannter First-Flight-Daten \u2013 also aller Merkmale, die direkt zu Beginn einer Netzwerkverbindung verf\u00fcgbar sind, z.\u202fB.:</p> <ul> <li>Zielport, Protokoll, TCP-Flags</li> <li>Payload-L\u00e4nge, Entropie, Keywords (z.\u202fB. \u201eGET\u201c, \u201eroot\u201c)</li> <li>Zeit zwischen Paketen, Sessiondauer</li> <li>Herkunfts-IP, ASN, geographische Lage</li> <li>H\u00e4ufigkeit und Art der Sessions in den letzten Minuten</li> </ul> <p>Diese Daten werden zu einem Feature-Vektor verarbeitet und bilden den Zustand (state) f\u00fcr das RL-Modell.</p>"},{"location":"documentation/rl/#reinforcement-learning-aufbau-mdp","title":"Reinforcement Learning Aufbau (MDP)","text":""},{"location":"documentation/rl/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<ul> <li>Agent: Das Decision-Modul zur Containerwahl</li> <li>Environment: Das Honeypot-System mit dynamischer Orchestrierung</li> <li>State (\ud835\udc60\u209c): Feature-Vektor der First-Flight-Daten</li> <li>Action (\ud835\udc4e\u209c): Auswahl eines Containers oder Verzicht (<code>Ignore</code>)</li> <li>Reward (\ud835\udc5f\u209c): Bewertung auf Basis von Session-Qualit\u00e4t und Ressourcenverbrauch</li> <li>Policy (\u03c0): Funktion zur Entscheidung (z.\u202fB. Deep Neural Network)</li> </ul>"},{"location":"documentation/rl/#moglicher-aktionsraum","title":"M\u00f6glicher Aktionsraum","text":"<p><pre><code>Actions = {\n  Deploy_HTTP,\n  Deploy_SSH,\n  Deploy_SMB,\n  Deploy_Telnet,\n  Deploy_MySQL,\n  Ignore\n}\n</code></pre> It might be better to make the decision of the service dependent on other methods then the RL: <pre><code>Actions = {\n  Deploy,\n  Ignore\n}\n</code></pre></p>"},{"location":"documentation/rl/#belohnungsfunktion","title":"\ud83d\udca1 Belohnungsfunktion","text":"<p>Die Belohnung ist so definiert, dass sie gute Entscheidungen f\u00f6rdert, die zu neuen, tiefen Interaktionen f\u00fchren \u2013 ohne Ressourcen zu verschwenden:</p> <pre><code>Reward = (Informationsgewinn / Containerkosten) - Redundanzstrafe\n</code></pre> <p>Komponenten: - Informationsgewinn: neue Payloads, Authentifizierungsversuche, Shell-Zugriffe etc. - Containerkosten: CPU-Zeit, RAM, Laufzeit - Redundanzstrafe: wenn Angreifer bereits bekannt / typisches Botverhalten</p>"},{"location":"documentation/rl/#modellwahl-q-learning-vs-deep-rl","title":"\ud83e\udde0 Modellwahl: Q-Learning vs. Deep RL","text":"<p>F\u00fcr einfache Proof-of-Concepts kann tabellarisches Q-Learning verwendet werden. F\u00fcr realistische Anwendungen mit kontinuierlichen, hochdimensionalen Zust\u00e4nden wird Deep Reinforcement Learning empfohlen (z.\u202fB. DQN oder PPO).</p>"},{"location":"documentation/rl/#ergebnis","title":"\ud83d\ude80 Ergebnis","text":"<p>Mit diesem Ansatz entsteht ein lernf\u00e4higes, ressourcenschonendes und realistisches Honeypot-System, das:</p> <ul> <li>gezielt eskalierende oder zielgerichtete Angreifer erkennt</li> <li>adaptive Emulation bietet (\u2192 schwerer zu enttarnen)</li> <li>Container dynamisch auf Nachfrage startet</li> <li>bekannte Bots ignoriert und neue Angriffsformen priorisiert</li> </ul>"},{"location":"documentation/thoughts/","title":"Thoughts and Ideas","text":"<p>This document serves as a structured place to record spontaneous thoughts, observations, and ideas related to the thesis.</p>"},{"location":"documentation/thoughts/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Thoughts and Ideas</li> <li>Table of Contents</li> <li>General Observations</li> <li>possible research gaps</li> <li>Questions to Clarify</li> <li>Potential Improvements</li> <li>Literature Insights</li> <li>Future Research Ideas</li> <li>To Investigate</li> </ul>"},{"location":"documentation/thoughts/#general-observations","title":"General Observations","text":"<p>Date: 9.7.2025 - I would like to demonstarte the attacker perspective in the paper. Date: 27.6.2025 - I must configure kubenretes to prepare pods for fast forward. so no classic deploy initialization Date: 20.6.2025 - adds to paper approch integrate full iac and devops approach Date: 14.6.2025 - Write in Paper about connection to game theory, many dqns are games and there are many papers about game Theory. I can focus deeply in paper on theoratically architecture. Date: 11.6.2025 - I want to focus on my introduction and the mistake that many papers expect human attackers instead of bots in most interactions. Then i want to describe the vision basen on this, detecting interesting connections and emulate deeply, also adaptiveoy which well known pod to spawn for example cowrie, maybe fork cowrie to make it more adaptive, maybe emulate command answers based on ML. Based on the deeper Logs, we can add recognition of attack chains if they follow the same pattern and can so detect bots, and if they change minimally, we can add versions to them. Also, with an autoencoder, we can detect anomalies in the logs. Also, an implementation can detect attack chains across multiple sessions, maybe an attacker is working with two different source IPs, not to get detected. Date: 19.5.2025 -  I should/could compare my honeynet with and without RL. Computing resources? Data? Date: 14.5.2025 -  How does my approach reacts for example to port scanners like nmap? Date: 24.3.2025 -  Question BSI, can i expect that the dataset has less then 5% Attack logs? Have there been analysis of the logs and is there a clean log subset Date: 21.3.2025 -  Nearly no paper is about anomaly detection in honeypots. That makes sense, because NIDS are the main usecase and there is no big difference between the both in my research context, but it should be possible to use this fact for my research gap Date: 19.3.2025 -  Metrics (Precision, Recall, ...) should be explained as formula. Date: 19.3.2025 -  The paper should cover basic math concepts, backpropagation might be to basic but in the explanation of teh Autoencoder or lstm it would fit really well. Date: 18.3.2025 -  The paper should mention the differences and parallels between IDS and Honeypot. The proposed solution could find application in both domains.</p>"},{"location":"documentation/thoughts/#possible-research-gaps","title":"possible research gaps","text":"<ol> <li>practical implementation of an Autoencoder with the specific BSI data. (Not enough theoratically work)</li> <li>integrating current aproaches but developing a solution to use the classified information to make the honeypot adaptive to the attacker to gather more information of its behaviour</li> <li>combining current approaches to one new solution (just first bytes, without preprocessing and feature extraction on raw data, system logs als language sequences, non-symmetric data dimensionality reduction, stacked NDAEs and the RF classification algorithm)</li> <li>Analyze logs on line, sessio and cross session level <ul> <li>Attacker behavior spans multiple sessions and sequences.  </li> <li>Logs are unstructured and not always tied to a single user or continuous flow.  </li> <li>Single-sequence modeling (like DeepLog) misses cross-session patterns.</li> <li>Proposed Solution: </li> <li>Line-level anomaly detection: Autoencoder or text reconstruction for individual log entries.  </li> <li>Intra-session modeling: LSTM to detect sequential anomalies within sessions.  </li> <li>Cross-session modeling:  <ul> <li>LSTM or Graph-based structure over session embeddings.  </li> <li>Capture patterns across multiple sessions over time.  </li> </ul> </li> <li>Global attention layer: Focus on key sessions or time periods.  </li> <li>Fusion layer: Combine all scores into one anomaly indicator.</li> </ul> </li> <li>be able to extract maybe with the lstm the attack chain or at least parts of the attacker to have a output of the honeypot which is usefull. Adaptiveness could build up on that data.</li> <li>the focus could also be on the prblem, that i have not a labeled dataset and thus, no clean training data</li> </ol>"},{"location":"documentation/thoughts/#questions-to-clarify","title":"Questions to Clarify","text":"<p>Date: 24.3.2025 -  Should i use byt5 es transformer at first step to big? Date: 22.3.2025 -  How can i use generative AI? If i write a text based on my thoughts and with referenced literature, can i input this text, for instance, to ChatGpt and let it rewrite it so its wriiten in a academic and typical journal article manner? Date: 21.3.2025 -  What is the BSI doing at the momant with the Honeypot Data? What analysis is happening? Date: 20.3.2025 -  Can i make the BSI Dataset public? Could it be transfered to a cloud provider? Date: 19.3.2025 -  How can i evaluate the performance of my model, if i do not have labeled test data? That restriction might influence the whole approach. Date: 19.3.2025 -  A typical journal paper ranges from 5 to 20 pages, while a regular bachelor's thesis is around 40-50 pages. Would a highly focused and concentrated 15-page paper, demonstrating significant research quality and depth, be sufficient as a bachelor's thesis?</p>"},{"location":"documentation/thoughts/#potential-improvements","title":"Potential Improvements","text":""},{"location":"documentation/thoughts/#literature-insights","title":"Literature Insights","text":""},{"location":"documentation/thoughts/#future-research-ideas","title":"Future Research Ideas","text":""},{"location":"documentation/thoughts/#to-investigate","title":"To Investigate","text":""}]}