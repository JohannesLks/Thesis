
# Adaptive Container-Orchestrierung mit Reinforcement Learning (RL)

## Motivation

Statische Honeypot-AnsÃ¤tze wie T-Pot bieten dauerhaft eine Vielzahl von Services auf festen Ports an â€“ unabhÃ¤ngig davon, ob ein Angreifer tatsÃ¤chlich mit ihnen interagieren mÃ¶chte. Diese Strategie erzeugt zwar groÃŸe Mengen an Daten, fÃ¼hrt jedoch zu:

- hoher Ressourcenlast durch permanent laufende Container
- reduzierter Tarnung (alle Ports offen = Honeypot-Indikator)
- vielen irrelevanten Sessions (z.â€¯B. durch einfache Scanner oder Bots)

Um diese Nachteile zu Ã¼berwinden, setzt unser System auf **adaptive Containerbereitstellung mithilfe von Reinforcement Learning (RL)**. Ziel ist es, nur dann einen Emulationscontainer zu starten, wenn **First-Flight-Daten** einer Quell-IP ein hohes Potenzial fÃ¼r wertvolle Angreiferinteraktionen aufweisen.

---

## âœ… Vorteile des RL-basierten Deployments gegenÃ¼ber statischen Systemen

| Kriterium                     | Statisches System (z.â€¯B. T-Pot) | RL-basiertes System              |
|------------------------------|----------------------------------|----------------------------------|
| Ressourceneffizienz          | Gering                           | Hoch                             |
| Tarnung                      | Gering (alle Ports offen)        | Hoch (on-demand-Emulation)       |
| DatenqualitÃ¤t                | Viel Bot-Rauschen                | Fokus auf wertvolle Interaktionen |
| Reaktion auf Angreiferverhalten | Keine                         | Host-spezifisch, lernfÃ¤hig       |
| Skalierbarkeit               | Begrenzt durch Last              | Dynamisch skalierbar             |

---

## Entscheidungsgrundlage: First-Flight-Daten

Der RL-Agent trifft seine Entscheidungen auf Basis sogenannter **First-Flight-Daten** â€“ also aller Merkmale, die direkt zu Beginn einer Netzwerkverbindung verfÃ¼gbar sind, z.â€¯B.:

- Zielport, Protokoll, TCP-Flags
- Payload-LÃ¤nge, Entropie, Keywords (z.â€¯B. â€GETâ€œ, â€rootâ€œ)
- Zeit zwischen Paketen, Sessiondauer
- Herkunfts-IP, ASN, geographische Lage
- HÃ¤ufigkeit und Art der Sessions in den letzten Minuten

Diese Daten werden zu einem Feature-Vektor verarbeitet und bilden den **Zustand (state)** fÃ¼r das RL-Modell.

---

##  Reinforcement Learning Aufbau (MDP)

### Markov Decision Process (MDP)

- **Agent**: Das Decision-Modul zur Containerwahl
- **Environment**: Das Honeypot-System mit dynamischer Orchestrierung
- **State (ğ‘ â‚œ)**: Feature-Vektor der First-Flight-Daten
- **Action (ğ‘â‚œ)**: Auswahl eines Containers oder Verzicht (`Ignore`)
- **Reward (ğ‘Ÿâ‚œ)**: Bewertung auf Basis von Session-QualitÃ¤t und Ressourcenverbrauch
- **Policy (Ï€)**: Funktion zur Entscheidung (z.â€¯B. Deep Neural Network)

### MÃ¶glicher Aktionsraum

```
Actions = {
  Deploy_HTTP,
  Deploy_SSH,
  Deploy_SMB,
  Deploy_Telnet,
  Deploy_MySQL,
  Ignore
}
```
It might be better to make the decision of the service dependent on other methods then the RL:
```
Actions = {
  Deploy,
  Ignore
}
```

---

## ğŸ’¡ Belohnungsfunktion

Die Belohnung ist so definiert, dass sie gute Entscheidungen fÃ¶rdert, die zu **neuen, tiefen Interaktionen** fÃ¼hren â€“ ohne Ressourcen zu verschwenden:

```math
Reward = (Informationsgewinn / Containerkosten) - Redundanzstrafe
```

**Komponenten:**
- Informationsgewinn: neue Payloads, Authentifizierungsversuche, Shell-Zugriffe etc.
- Containerkosten: CPU-Zeit, RAM, Laufzeit
- Redundanzstrafe: wenn Angreifer bereits bekannt / typisches Botverhalten

---

## ğŸ§  Modellwahl: Q-Learning vs. Deep RL

FÃ¼r einfache Proof-of-Concepts kann tabellarisches Q-Learning verwendet werden.  
FÃ¼r realistische Anwendungen mit kontinuierlichen, hochdimensionalen ZustÃ¤nden wird **Deep Reinforcement Learning** empfohlen (z.â€¯B. DQN oder PPO).

---

## ğŸš€ Ergebnis

Mit diesem Ansatz entsteht ein **lernfÃ¤higes, ressourcenschonendes und realistisches Honeypot-System**, das:

- gezielt eskalierende oder zielgerichtete Angreifer erkennt
- adaptive Emulation bietet (â†’ schwerer zu enttarnen)
- Container dynamisch auf Nachfrage startet
- bekannte Bots ignoriert und neue Angriffsformen priorisiert

---
