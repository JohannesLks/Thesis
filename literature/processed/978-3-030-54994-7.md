# Summary of "Formal Methods: FM 2019 International Workshops"

This document provides a detailed summary of the key sections and papers presented in the proceedings of the FM 2019 International Workshops, published as LNCS 12232.

## Part I Overview

The proceedings cover a wide range of topics in formal methods, organized into several workshops:

- **AFFORD 2019**: 3rd Workshop on Practical Formal Verification for Software Dependability
- **DataMod 2019**: 8th International Symposium From Data to Models and Back
- **FMAS 2019**: 1st Formal Methods for Autonomous Systems Workshop
- **FMBC 2019**: 1st Workshop on Formal Methods for Blockchains
- **FMIS 2019**: 8th Formal Methods for Interactive Systems Workshop

## Detailed Summary of "Anomaly Detection from Log Files Using Unsupervised Deep Learning"

This paper, authored by Sathya Bursic, Vittorio Cuculo, and Alessandro Dâ€™Amelio, presents a novel approach for detecting anomalies in system log files using an unsupervised deep learning model.

### 1. Introduction & Motivation

- **Problem**: Modern computer systems are complex and generate vast amounts of log data, making manual inspection for anomalies impractical.
- **Existing Solutions**: Many existing automated solutions rely on hand-crafted features, require significant preprocessing, or use supervised learning models which need labeled datasets (often unavailable).
- **Proposed Solution**: The authors propose a two-part deep autoencoder model with LSTM units that works on raw text and does not require hand-crafted features. It outputs an anomaly score for each log entry, reflecting the rarity of the log event in terms of both content and temporal context.

### 2. Related Work

The paper reviews traditional three-step log anomaly detection:
1.  **Log Parsing**: Converting unstructured text to structured data (clustering-based or heuristic-based).
2.  **Feature Extraction**: Transforming text into numerical vectors.
3.  **Anomaly Detection**: Applying machine learning algorithms (supervised like SVM, or unsupervised like PCA).

Recent deep learning approaches are also discussed, noting their superior performance, including LSTM-based models for learning log patterns and autoencoders for detecting anomalies based on reconstruction errors.

### 3. Proposed Method

- **Model Architecture**:
    1.  An initial autoencoder embeds variable-length log messages into a fixed-dimensional vector space. This is trained on the log text (without timestamps).
    2.  A second LSTM autoencoder is trained for anomaly detection. It takes the message embeddings and a numerical timestamp as input.
    3.  An anomaly score is calculated based on the distance between the inputs to the second autoencoder and their reconstructed outputs.

- **Implementation and Training**:
    - **Dataset**: A subset of a Hadoop File System (HDFS) log dataset with 2 million lines (1M for training, 1M for testing) is used. The dataset is labeled, which is useful for evaluation.
    - **Preprocessing**: Minimal preprocessing is performed, mainly separating date/time from the message, replacing non-alphanumeric characters with whitespace, and tokenizing numbers.
    - **Training**:
        - The text embedding autoencoder uses a word embedding size of 200 and 100 LSTM units.
        - The anomaly detection autoencoder uses 64 LSTM units and takes the message embedding plus a cosine-transformed timestamp (to capture daily cyclical patterns).
        - The cost function for training is Mean Squared Error (MSE), and the reconstruction error for anomaly detection is the L1 distance.

### 4. Experimental Results

- **Anomaly Scores**: A plot of anomaly scores on the test set shows several outliers with high scores, but most values are clustered, suggesting the need for careful threshold selection.
- **Performance Evaluation**:
    - The problem is treated as a binary classification task (anomalous vs. not anomalous).
    - An ROC curve is generated, showing an AUC of 0.59, indicating modest classifier performance.
    - A precision, recall, and F-score analysis reveals that at a threshold of 62, the F-score is maximized. Beyond this, precision rises while recall falls sharply.
- **Conclusion on Results**:
    - The model has a consistently high number of false negatives, meaning many anomalies are missed.
    - However, at higher thresholds, false positives decrease faster than true positives, leading to high precision. This means that the anomalies it does detect are very likely to be genuine.

### 5. Conclusion

- The proposed unsupervised deep learning model can be a useful tool as a coarse filter for manual inspection of log files, especially when a labeled dataset is not available.
- By choosing a high threshold, the system can identify high-confidence anomalies, reducing the amount of data a human needs to inspect, despite missing many other anomalies.
- The model's key advantage is its ability to work on generic raw log data with minimal preprocessing.

---

This summary has been created by Roo, a highly skilled software engineer.